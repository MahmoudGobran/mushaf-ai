"""
Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© ÙˆÙ…Ø¯Ù…Ø¬Ø© (Ù…ØµØ­Ø­Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„)
âœ… Ø¯Ø¹Ù… Ø£ÙØ¶Ù„ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© ÙˆØ§Ù„Ø±Ø³Ù… Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ
âœ… ØªÙˆØ­ÙŠØ¯ Ø´Ø§Ù…Ù„ Ù„Ù„Ù‡Ù…Ø²Ø§Øª ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
âœ… Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
âœ… Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø°ÙƒØ± Ø§Ù„Ø³Ø§Ù„Ù…
âœ… Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
âœ… Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ¸Ù„ÙŠÙ„ - "Ø¢ØªÙˆØ§ Ø§Ù„Ø²ÙƒØ§Ø©"
âœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
"""

import re
from difflib import SequenceMatcher
from typing import List, Tuple, Dict, Any
import unicodedata

# ============================================
# ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© ÙˆÙ…Ø¯Ù…Ø¬Ø©
# ============================================

def normalize_arabic_text(text: str) -> str:
    """
    Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ - ÙŠØ·Ø§Ø¨Ù‚ textNormalizer.js Ø¨Ø§Ù„Ø¶Ø¨Ø·
    
    Args:
        text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ·Ø¨ÙŠØ¹Ù‡
        
    Returns:
        Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙƒØ§Ù…Ù„
    """
    if not text:
        return ""
    
    normalized = text
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”¥ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø®Ø§ØµØ©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 0.0 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªØ­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹ (Ù„Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…)
    normalized = re.sub(r'Ø¥ÙØ¨Ù’Ø±ÙÙ°Ù‡ÙÛ¦Ù…Ù', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', normalized) 
    normalized = re.sub(r'Ø¥ÙØ¨Ù’Ø±ÙÙ°Ù‡ÙÙŠÙ…Ù', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', normalized)
    normalized = re.sub(r'Ø¥ÙØ¨Ù’Ø±ÙØ§Ù‡ÙÙŠÙ…Ù', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', normalized)
    normalized = re.sub(r'Ø¥ÙØ¨Ù’Ø±ÙÙ°Ù‡ÙÙ…Ù', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', normalized)
    
    # 0.1 Ù…Ø¹Ø§Ù„Ø¬Ø© "Ù„ÙƒÙ†" Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ - ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯Ø©
    normalized = re.sub(r'Ù„ÙÙ°ÙƒÙÙ†Ù‘Ù?', 'Ù„ÙƒÙ†', normalized)
    
    # 0.2 ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù„Ù Ø§Ù„Ø®Ù†Ø¬Ø±ÙŠØ© Ø¥Ù„Ù‰ Ø£Ù„Ù
    normalized = re.sub(r'Ù°', 'Ø§', normalized) # Ù° -> Ø§

    # 0.3 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‡Ù…Ø²Ø© Ø§Ù„Ù…Ø¶Ù…ÙˆÙ…Ø© ÙˆØ§Ù„Ù‡Ù…Ø²Ø© Ø§Ù„ÙˆØµÙ„ (Ù±)
    normalized = re.sub(r'Ù±', 'Ø§', normalized)  # Ù‡Ù…Ø²Ø© Ø§Ù„ÙˆØµÙ„ â†’ Ø£Ù„Ù
    normalized = re.sub(r'Ø¥Ù', 'Ø§', normalized)  # Ù‡Ù…Ø²Ø© Ù…ÙƒØ³ÙˆØ±Ø© â†’ Ø£Ù„Ù
    normalized = re.sub(r'Ø£Ù', 'Ø§', normalized)  # Ù‡Ù…Ø²Ø© Ù…ÙØªÙˆØ­Ø© â†’ Ø£Ù„Ù
    normalized = re.sub(r'Ø£Ù', 'Ø§', normalized)  # Ù‡Ù…Ø²Ø© Ù…Ø¶Ù…ÙˆÙ…Ø© â†’ Ø£Ù„Ù
    
    # 0.4 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙŠØ§Ø¡ Ø§Ù„ØµØºÙŠØ±Ø© ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙÙˆÙ‚ Ø§Ù„Ø­Ø±ÙˆÙ
    normalized = re.sub(r'Û¦', '', normalized)  # ÙŠØ§Ø¡ ØµØºÙŠØ±Ø©
    normalized = re.sub(r'Û¥', '', normalized)  # ÙˆØ§Ùˆ ØµØºÙŠØ±Ø©
    normalized = re.sub(r'Û¢', '', normalized)  # Ø¹Ù„Ø§Ù…Ø§Øª Ù‚Ø±Ø¢Ù†ÙŠØ©
    normalized = re.sub(r'Û ', '', normalized)
    
    # 0.5 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ù„Ù Ø§Ù„Ø®Ù†Ø¬Ø±ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
    normalized = re.sub(r'([Ø¨ØªØ«Ø¬Ø­Ø®Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ])ÙÙ°', r'\1Ø§', normalized)  # Ø­Ø±ÙÙÙ° â†’ Ø­Ø±ÙØ§
    normalized = re.sub(r'([Ø¨ØªØ«Ø¬Ø­Ø®Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ])ÙÙ°', r'\1ÙŠ', normalized)  # Ø­Ø±ÙÙÙ° â†’ Ø­Ø±ÙÙŠ
    normalized = re.sub(r'([Ø¨ØªØ«Ø¬Ø­Ø®Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ])ÙÙ°', r'\1Ùˆ', normalized)  # Ø­Ø±ÙÙÙ° â†’ Ø­Ø±ÙÙˆ
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„ØªØ´ÙƒÙŠÙ„ (Ø´Ø§Ù…Ù„Ø© 100%)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 1.1 Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    normalized = re.sub(r'[\u064B-\u065F]', '', normalized)  # Ù‹ ÙŒ Ù Ù Ù Ù Ù‘ Ù’ Ù“ Ù” Ù•
    
    # 1.2 Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¯ ÙˆØ§Ù„ÙˆÙ‚Ù Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ©
    normalized = re.sub(r'[\u0610-\u061A]', '', normalized)  # Ø Ø‘ Ø’ Ø“ Ø” Ø•
    normalized = re.sub(r'[\u06D6-\u06ED]', '', normalized)  # Û– Û— Û˜ Û™ Ûš Û› Ûœ Û Û
    
    # 1.3 Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…ÙˆØ³Ø¹Ø©
    normalized = re.sub(r'[\u08D3-\u08E1]', '', normalized)  # Ø¹Ù„Ø§Ù…Ø§Øª Ù‚Ø±Ø¢Ù†ÙŠØ© Ù…ÙˆØ³Ø¹Ø©
    normalized = re.sub(r'[\u08E3-\u08FF]', '', normalized)  # Ø¹Ù„Ø§Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    
    # 1.4 Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙˆØ§Ùˆ ÙˆØ§Ù„ÙŠØ§Ø¡ Ø§Ù„ØµØºÙŠØ±Ø©
    normalized = re.sub(r'[Û¥Û¦Û­]', '', normalized)
    
    # 1.5 Ø¥Ø²Ø§Ù„Ø© Tatweel
    normalized = re.sub(r'[\u0640]', '', normalized)         # Ù€
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª ÙˆØ§Ù„Ø£Ù„Ù (Ù…Ø­Ø³Ù†Ø©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 2.1 ØªÙˆØ­ÙŠØ¯ ÙƒÙ„ Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø£Ù„Ù (Ø´Ø§Ù…Ù„Ø© Ø£ÙƒØ«Ø±)
    normalized = re.sub(r'[Ø£Ø¥Ø¢Ù±Ø¡]', 'Ø§', normalized)  # Ø£ Ø¥ Ø¢ Ù± Ø¡ â†’ Ø§
    
    # 2.2 Ø§Ù„Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ§Ùˆ ÙˆØ§Ù„ÙŠØ§Ø¡
    normalized = re.sub(r'Ø¤', 'Ùˆ', normalized)
    normalized = re.sub(r'Ø¦', 'ÙŠ', normalized)
    normalized = re.sub(r'Ùµ', 'Ø§', normalized)  # Ø£Ù„Ù Ù…Ø¹ Ù‡Ù…Ø²Ø© ÙƒØ¨ÙŠØ±Ø©
    normalized = re.sub(r'Ù²', 'Ø§', normalized)  # Ø£Ù„Ù Ù…Ø¹ Ù‡Ù…Ø²Ø© Ù…Ø§Ø¦Ù„Ø©
    
    # 2.3 Ø§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø© ÙˆØ§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
    normalized = re.sub(r'[Ù‰]', 'ÙŠ', normalized)  # Ù‰ â†’ ÙŠ
    normalized = re.sub(r'Ø©', 'Ù‡', normalized)    # Ø© â†’ Ù‡
    normalized = re.sub(r'Ûƒ', 'Ù‡', normalized)    # ØªØ§Ø¡ Ù…Ø±Ø¨ÙˆØ·Ø© Ø¨Ø¯ÙŠÙ„Ø©
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”¥ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© (Ù…ÙØµÙØ­ÙÙ‘Ø­Ø©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 3.1 Ø§Ù„Ø£Ù„Ù Ø§Ù„Ø®Ù†Ø¬Ø±ÙŠØ©
    normalized = re.sub(r'Ù°', 'Ø§', normalized)
    
    # ğŸ”¥ 3.2 Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© "Ø§Ù„Ø±Ø¨Ø§" - "Ø±Ø¨ÙˆØ§" ÙŠØ¬Ø¨ Ø£Ù† ØªØµØ¨Ø­ "Ø±Ø¨Ø§"
    normalized = re.sub(r'Ø±ÙØ¨ÙÙˆÙØ§', 'Ø±Ø¨Ø§', normalized)  # Ø±ÙØ¨ÙÙˆÙØ§ â†’ Ø±Ø¨Ø§
    normalized = re.sub(r'Ø±Ø¨ÙˆØ§', 'Ø±Ø¨Ø§', normalized)     # Ø±Ø¨ÙˆØ§ â†’ Ø±Ø¨Ø§
    
    # ğŸ”¥ 3.3 ØªØ­ÙˆÙŠÙ„ "ÙˆØ©" Ø¥Ù„Ù‰ "Ø§Ù‡" (Ù‚ÙˆØ§Ø¹Ø¯ Ù…ÙˆØ³Ø¹Ø©)
    normalized = re.sub(r'(\S)ÙˆØ©(\s|$)', r'\1Ø§Ù‡\2', normalized)
    normalized = re.sub(r'(\s)ÙˆØ©(\s)', r'\1Ø§Ù‡\2', normalized)
    normalized = re.sub(r'^ÙˆØ©(\s)', r'Ø§Ù‡\1', normalized)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø§ØµØ©
    normalized = re.sub(r'ØµÙ„ÙˆØ§Ù‡', 'ØµÙ„Ø§Ù‡', normalized)
    normalized = re.sub(r'Ø²ÙƒÙˆØ§Ù‡', 'Ø²ÙƒØ§Ù‡', normalized)
    normalized = re.sub(r'Ø­ÙŠÙˆÙ‡', 'Ø­ÙŠØ§Ù‡', normalized)
    normalized = re.sub(r'Ù…Ù†ÙˆÙ‡', 'Ù…Ù†Ø§Ù‡', normalized)
    
    # 3.4 Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø®Ø§ØµØ©
    normalized = re.sub(r'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', 'Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…', normalized)
    normalized = re.sub(r'Ø³Ù„ÙŠÙ…Ø§Ù†', 'Ø³Ù„ÙŠÙ…Ø§Ù†', normalized)
    normalized = re.sub(r'Ø¹Ù…Ø±Ø§Ù†', 'Ø¹Ù…Ø±Ø§Ù†', normalized)
    
    # 3.5 Ø§Ù„Ù‡Ù…Ø²Ø© Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø£ÙØ¹Ø§Ù„
    normalized = re.sub(r'Ø§Ø§Ù…Ù†', 'Ø§Ù…Ù†', normalized)
    normalized = re.sub(r'Ø§Ø§ØªÙˆØ§', 'Ø§ØªÙˆØ§', normalized)
    normalized = re.sub(r'Ø§Ø§ØªÙŠ', 'Ø§ØªÙŠ', normalized)
    
    # 3.6 Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ø­Ø±ÙˆÙ
    normalized = re.sub(r'Ø§ÙˆÙ„Ø§Ø¦Ùƒ', 'Ø§ÙˆÙ„Ø¦Ùƒ', normalized)
    normalized = re.sub(r'Ø§ÙˆÙ„ÙˆØ§', 'Ø§ÙˆÙ„Ùˆ', normalized)
    normalized = re.sub(r'Ù‡Ø§Ø°Ø§', 'Ù‡Ø°Ø§', normalized)
    normalized = re.sub(r'Ø°Ø§Ù„Ùƒ', 'Ø°Ù„Ùƒ', normalized)
    
    # 3.7 Ø§Ù„Ù†Ø¯Ø§Ø¡
    normalized = re.sub(r'ÙŠØ§ ?Ø§ÙŠÙ‡Ø§', 'ÙŠØ§ Ø§ÙŠÙ‡Ø§', normalized)
    normalized = re.sub(r'ÙŠØ§ ?Ø¨Ù†ÙŠ', 'ÙŠØ§ Ø¨Ù†ÙŠ', normalized)
    
    # 3.8 "Ø§Ù„Ø°ÙŠÙ†"
    normalized = re.sub(r'Ø§Ù„Ø§Ø°ÙŠÙ†', 'Ø§Ù„Ø°ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ø°ÙŠÙ†', normalized)
    
    # 3.9 Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ù…Ù† Ø³ÙˆØ±Ø© Ø§Ù„Ø£Ø­Ø²Ø§Ø¨ ÙˆØ§Ù„Ø¬Ù…ÙˆØ¹
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØ³Ù’Ù„ÙÙ…ÙÙŠÙ†Ù', 'Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØ³Ù’Ù„ÙÙ…ÙÙ°ØªÙ', 'Ø§Ù„Ù…Ø³Ù„Ù…Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØ¤Ù’Ù…ÙÙ†ÙÙŠÙ†Ù', 'Ø§Ù„Ù…ÙˆÙ…Ù†ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØ¤Ù’Ù…ÙÙ†ÙÙ°ØªÙ', 'Ø§Ù„Ù…ÙˆÙ…Ù†Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù‚ÙÙ°Ù†ÙØªÙÙŠÙ†Ù', 'Ø§Ù„Ù‚Ø§Ù†ØªÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù‚ÙÙ°Ù†ÙØªÙÙ°ØªÙ', 'Ø§Ù„Ù‚Ø§Ù†ØªØ§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ø¯ÙÙ‚ÙÙŠÙ†Ù', 'Ø§Ù„ØµØ§Ø¯Ù‚ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ø¯ÙÙ‚ÙÙ°ØªÙ', 'Ø§Ù„ØµØ§Ø¯Ù‚Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ø¨ÙØ±ÙÙŠÙ†Ù', 'Ø§Ù„ØµØ§Ø¨Ø±ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ø¨ÙØ±ÙÙ°ØªÙ', 'Ø§Ù„ØµØ§Ø¨Ø±Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø®ÙÙ°Ø´ÙØ¹ÙÙŠÙ†Ù', 'Ø§Ù„Ø®Ø§Ø´Ø¹ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø®ÙÙ°Ø´ÙØ¹ÙÙ°ØªÙ', 'Ø§Ù„Ø®Ø§Ø´Ø¹Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØªÙØµÙØ¯Ù‘ÙÙ‚ÙÙŠÙ†Ù', 'Ø§Ù„Ù…ØªØµØ¯Ù‚ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ù…ÙØªÙØµÙØ¯Ù‘ÙÙ‚ÙÙ°ØªÙ', 'Ø§Ù„Ù…ØªØµØ¯Ù‚Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ù“Ø¦ÙÙ…ÙÙŠÙ†Ù', 'Ø§Ù„ØµØ§Ø¦Ù…ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„ØµÙ‘ÙÙ°Ù“Ø¦ÙÙ…ÙÙ°ØªÙ', 'Ø§Ù„ØµØ§Ø¦Ù…Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø­ÙÙ°ÙÙØ¸ÙÙŠÙ†Ù', 'Ø§Ù„Ø­Ø§ÙØ¸ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø­ÙÙ°ÙÙØ¸ÙÙ°ØªÙ', 'Ø§Ù„Ø­Ø§ÙØ¸Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ø°Ù‘ÙÙ°ÙƒÙØ±ÙÙŠÙ†Ù', 'Ø§Ù„Ø°Ø§ÙƒØ±ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ø°Ù‘ÙÙ°ÙƒÙØ±ÙÙ°ØªÙ', 'Ø§Ù„Ø°Ø§ÙƒØ±Ø§Øª', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø®ÙÙ°Ø³ÙØ±ÙÙŠÙ†Ù', 'Ø§Ù„Ø®Ø§Ø³Ø±ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’Ø®ÙÙ°Ø³ÙØ±ÙÙˆÙ†Ù', 'Ø§Ù„Ø®Ø§Ø³Ø±ÙˆÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’ÙÙÙ°Ø³ÙÙ‚ÙÙŠÙ†Ù', 'Ø§Ù„ÙØ§Ø³Ù‚ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’ÙÙÙ°Ø³ÙÙ‚ÙÙˆÙ†Ù', 'Ø§Ù„ÙØ§Ø³Ù‚ÙˆÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ø¸Ù‘ÙÙ°Ù„ÙÙ…ÙÙŠÙ†Ù', 'Ø§Ù„Ø¸Ø§Ù„Ù…ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ø¸Ù‘ÙÙ°Ù„ÙÙ…ÙÙˆÙ†Ù', 'Ø§Ù„Ø¸Ø§Ù„Ù…ÙˆÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’ÙƒÙÙ°ÙÙØ±ÙÙŠÙ†Ù', 'Ø§Ù„ÙƒØ§ÙØ±ÙŠÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„Ù’ÙƒÙÙ°ÙÙØ±ÙÙˆÙ†Ù', 'Ø§Ù„ÙƒØ§ÙØ±ÙˆÙ†', normalized)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ù…ÙˆØ¹ ÙˆØ§Ù„ØµÙŠØº Ø§Ù„Ù†Ø­ÙˆÙŠØ©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 4.1 Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø°ÙƒØ± Ø§Ù„Ø³Ø§Ù„Ù…
    normalized = re.sub(r'Ø§Ù„([Ø§-ÙŠ]{3,})ÙˆÙ†', r'Ø§Ù„\1ÙˆÙ†', normalized)
    normalized = re.sub(r'Ø§Ù„([Ø§-ÙŠ]{3,})ÙŠÙ†', r'Ø§Ù„\1ÙŠÙ†', normalized)
    
    # 4.2 Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¤Ù†Ø« Ø§Ù„Ø³Ø§Ù„Ù…
    normalized = re.sub(r'Ø§Ù„([Ø§-ÙŠ]{3,})Ø§Øª', r'Ø§Ù„\1Ø§Øª', normalized)
    
    # 4.3 ØµÙŠØº Ø§Ù„Ø£ÙØ¹Ø§Ù„
    normalized = re.sub(r'ÙŠÙˆ?Ù…Ù†', 'ÙŠÙˆÙ…Ù†', normalized)
    normalized = re.sub(r'ÙŠÙˆ?ØªÙŠ', 'ÙŠÙˆØªÙŠ', normalized)
    normalized = re.sub(r'ÙŠÙˆ?ØªÙ‰', 'ÙŠÙˆØªÙ‰', normalized)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 5.1 Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
    normalized = re.sub(r'[.,ØŒØ›!?ØŸ:\-()\[\]{}"\'Â«Â»]', '', normalized)
    
    # 5.2 Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    normalized = re.sub(r'[0-9Ù -Ù©]', '', normalized)
    
    # 5.3 ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # 5.4 Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    normalized = normalized.strip().lower()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”¥ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªØµØ­ÙŠØ­ Ù†Ù‡Ø§Ø¦ÙŠ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©
    normalized = re.sub(r'Ø§Ø§+', 'Ø§', normalized)
    normalized = re.sub(r'ÙŠÙŠ+', 'ÙŠ', normalized)
    normalized = re.sub(r'ÙˆÙˆ+', 'Ùˆ', normalized)
    normalized = re.sub(r'Ù‡Ù‡+', 'Ù‡', normalized) 
    normalized = re.sub(r'  +', ' ', normalized)
    
    # ğŸ”¥ 6.1 ØªØµØ­ÙŠØ­ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø­Ø§Ù„Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
    normalized = re.sub(r'Ø±Ø¨ÙˆØ§', 'Ø±Ø¨Ø§', normalized)  # ØªØ£ÙƒÙŠØ¯ ØªØ­ÙˆÙŠÙ„ "Ø±Ø¨ÙˆØ§" Ø¥Ù„Ù‰ "Ø±Ø¨Ø§"
    normalized = re.sub(r'Ù„Ø§ÙƒÙ†', 'Ù„ÙƒÙ†', normalized)  # ØªØ­ÙˆÙŠÙ„ "Ù„Ø§ÙƒÙ†" Ø¥Ù„Ù‰ "Ù„ÙƒÙ†"
    
    return normalized

# ğŸ”§ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
def clean_text(text: str) -> str:
    """
    Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ)
    ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    """
    return normalize_arabic_text(text)

# ============================================
# ğŸ¨ Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©: ØªØ¸Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù†Øµ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
# ============================================

def highlight_words_in_text(text: str, search_query: str) -> str:
    """
    âœ… ØªØ¸Ù„ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¨Ø­ÙˆØ« Ø¹Ù†Ù‡Ø§ ÙÙ‚Ø·
    
    Args:
        text: Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¢ÙŠØ©)
        search_query: Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (Ù…Ø§ Ø¨Ø­Ø« Ø¹Ù†Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
        
    Returns:
        Ù†Øµ Ù…Ø¹ ØªØ¸Ù„ÙŠÙ„ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙ‚Ø·
    """
    if not text or not search_query:
        return text
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    clean_query = normalize_arabic_text(search_query)
    clean_text_content = normalize_arabic_text(text)
    
    # âœ… Ø§Ù„Ø­Ø§Ù„Ø© 1: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø© (Ø£ÙƒØ«Ø± Ù…Ù† ÙƒÙ„Ù…Ø©)
    if ' ' in search_query.strip():
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        query_words = search_query.strip().split()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (Ø­Ø±Ù ÙˆØ§Ø­Ø¯)
        query_words = [w for w in query_words if len(w) > 1]
        
        if not query_words:
            return text
        
        # âœ… Ù†Ø¸Ù„Ù„ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚
        highlighted_text = text
        
        for word in query_words:
            word_clean = normalize_arabic_text(word)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
            text_words = highlighted_text.split()
            new_words = []
            
            for text_word in text_words:
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¸Ù„Ù„Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
                if '<mark>' in text_word or '</mark>' in text_word:
                    new_words.append(text_word)
                    continue
                
                # ØªÙ†Ø¸ÙŠÙ ÙƒÙ„Ù…Ø© Ø§Ù„Ù†Øµ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
                text_word_clean = normalize_arabic_text(text_word)
                
                # âœ… ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ ÙÙ‚Ø·
                if text_word_clean == word_clean:
                    new_words.append(f'<mark>{text_word}</mark>')
                else:
                    new_words.append(text_word)
            
            highlighted_text = ' '.join(new_words)
        
        return highlighted_text
    
    # âœ… Ø§Ù„Ø­Ø§Ù„Ø© 2: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©
    else:
        query_clean = clean_query.strip()
        
        if len(query_clean) < 2:
            return text
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        words = text.split()
        highlighted_words = []
        
        for word in words:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            word_clean = normalize_arabic_text(word)
            
            # âœ… ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ ÙÙ‚Ø· (Ù…Ø³Ø§ÙˆØ§Ø© ÙƒØ§Ù…Ù„Ø©)
            if word_clean == query_clean:
                highlighted_words.append(f'<mark>{word}</mark>')
            else:
                highlighted_words.append(word)
        
        return ' '.join(highlighted_words)
    
# ============================================
# ğŸ” Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
# ============================================

def calculate_similarity(text1: str, text2: str, use_words: bool = True) -> float:
    """
    Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù†ØµÙŠÙ† Ù…Ø¹ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø­Ø³Ù†
    
    Args:
        text1: Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„
        text2: Ø§Ù„Ù†Øµ Ø§Ù„Ø«Ø§Ù†ÙŠ
        use_words: Ø¥Ø°Ø§ TrueØŒ ÙŠÙ‚Ø§Ø±Ù† Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª. Ø¥Ø°Ø§ FalseØŒ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø­Ø±Ù
    
    Returns:
        Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (0.0 - 1.0)
    """
    clean1 = normalize_arabic_text(text1)
    clean2 = normalize_arabic_text(text2)
    
    if not clean1 or not clean2:
        return 0.0
    
    if use_words:
        # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Ø£Ø¯Ù‚)
        words1 = clean1.split()
        words2 = clean2.split()
        return SequenceMatcher(None, words1, words2).ratio()
    else:
        # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø­Ø±Ù
        return SequenceMatcher(None, clean1, clean2).ratio()

# ============================================
# ğŸ”§ Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
# ============================================

def normalize_search_query(query: str) -> str:
    """
    ØªØ·Ø¨ÙŠØ¹ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ù„Ø¯Ø¹Ù… Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© ÙˆØ§Ù„Ø±Ø³Ù… Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ
    """
    return normalize_arabic_text(query)

def get_most_common_words(verses: List[Dict], top_n: int = 5, exclude_common: bool = True) -> List[Dict]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª
    
    Args:
        verses: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢ÙŠØ§Øª
        top_n: Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        exclude_common: Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± etc)
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
    """
    # ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§
    common_words = {
        'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ø£Ù†', 'Ø¥Ù†', 'Ù…Ø§', 'Ù„Ø§', 'Ù‡Ù„', 'Ø¨Ù„',
        'Ù‚Ø¯', 'Ø³Ù‰', 'ÙƒØ§Ù†', 'ÙŠÙƒÙˆÙ†', 'Ù‚Ø§Ù„', 'Ù‚Ù„', 'Ø¥Ù†', 'Ø£Ù†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ù‡Ù…',
        'ÙƒØ°Ù„Ùƒ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø°Ù„Ùƒ', 'Ù‡Ø°Ù‡',
        'Ù‡Ø°Ø§', 'Ù‡Ø¤Ù„Ø§Ø¡', 'ØªÙ„Ùƒ', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø¨Ø¹Ø¶', 'ÙƒÙ„', 'Ø¬Ù…ÙŠØ¹', 'Ø£ÙŠ', 'Ø£ÙŠÙ†',
        'Ù…ØªÙ‰', 'ÙƒÙŠÙ', 'Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙ…', 'Ø£ÙŠØ¶Ø§', 'Ø«Ù…', 'Ø­ØªÙ‰', 'Ø£Ù…Ø§', 'Ø£Ùˆ', 'Ùˆ'
    }
    
    word_count = {}
    
    for verse in verses:
        text = normalize_arabic_text(verse.get('text', ''))
        words = text.split()
        
        for word in words:
            if len(word) < 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ù†ÙØ±Ø¯Ø©
                continue
                
            if exclude_common and word in common_words:
                continue
                
            word_count[word] = word_count.get(word, 0) + 1
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
    
    return [{'word': word, 'count': count} for word, count in sorted_words[:top_n]]

def get_most_common_phrases(verses: List[Dict], top_n: int = 5, phrase_length: int = 3) -> List[Dict]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª
    
    Args:
        verses: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢ÙŠØ§Øª
        top_n: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        phrase_length: Ø·ÙˆÙ„ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© (Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª)
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
    """
    phrase_count = {}
    
    for verse in verses:
        text = normalize_arabic_text(verse.get('text', ''))
        words = text.split()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª
        for i in range(len(words) - phrase_length + 1):
            phrase = ' '.join(words[i:i + phrase_length])
            
            if len(phrase.strip()) < phrase_length * 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                continue
                
            phrase_count[phrase] = phrase_count.get(phrase, 0) + 1
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    sorted_phrases = sorted(phrase_count.items(), key=lambda x: x[1], reverse=True)
    
    return [{'phrase': phrase, 'count': count} for phrase, count in sorted_phrases[:top_n]]

# ============================================
# ğŸ¨ ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª (Ù†ÙØ³ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
# ============================================

def highlight_differences(text1: str, text2: str) -> Tuple[List[dict], List[dict]]:
    """
    ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª Ø¨ÙŠÙ† Ù†ØµÙŠÙ†
    """
    clean1 = normalize_arabic_text(text1)
    clean2 = normalize_arabic_text(text2)
    
    words1 = clean1.split()
    words2 = clean2.split()
    
    matcher = SequenceMatcher(None, words1, words2)
    
    highlighted1 = []
    highlighted2 = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            for i in range(i1, i2):
                highlighted1.append({'type': 'same', 'text': words1[i]})
            for j in range(j1, j2):
                highlighted2.append({'type': 'same', 'text': words2[j]})
        
        elif tag == 'replace':
            for i in range(i1, i2):
                highlighted1.append({'type': 'diff', 'text': words1[i]})
            for j in range(j1, j2):
                highlighted2.append({'type': 'diff', 'text': words2[j]})
        
        elif tag == 'delete':
            for i in range(i1, i2):
                highlighted1.append({'type': 'diff', 'text': words1[i]})
        
        elif tag == 'insert':
            for j in range(j1, j2):
                highlighted2.append({'type': 'diff', 'text': words2[j]})
    
    return highlighted1, highlighted2

# ============================================
# ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
# ============================================

if __name__ == "__main__":
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø­Ø³Ù†:")
    
    test_cases = [
        # (Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©, Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ, Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹)
        ("Ø§Ù„ØµÙ„Ø§Ø©", "Ù±Ù„ØµÙÙ‘Ù„ÙÙˆÙ°Ø©Ù", "Ø§Ù„ØµÙ„Ø§Ù‡"),
        ("Ø§Ù„Ø²ÙƒØ§Ø©", "Ù±Ù„Ø²ÙÙ‘ÙƒÙÙˆÙ°Ø©Ù", "Ø§Ù„Ø²ÙƒØ§Ù‡"),
        ("ÙˆØ¢ØªÙˆØ§", "ÙˆÙØ¡ÙØ§ØªÙÙˆØ§Ù’", "ÙˆØ§ØªÙˆØ§"),
        ("Ø§Ù„Ø±Ø¨Ø§", "Ù±Ù„Ø±ÙÙ‘Ø¨ÙÙˆÙ°Ø§Ù’", "Ø§Ù„Ø±Ø¨Ø§"),
        ("Ø§Ù„Ø¸Ø§Ù„Ù…ÙŠÙ†", "Ù±Ù„Ø¸ÙÙ‘Ù°Ù„ÙÙ…ÙÙŠÙ†Ù", "Ø§Ù„Ø¸Ø§Ù„Ù…ÙŠÙ†"),
        ("Ø§Ù„ÙƒØ§ÙØ±ÙŠÙ†", "Ù±Ù„Û¡ÙƒÙÙ°ÙÙØ±ÙÙŠÙ†Ù", "Ø§Ù„ÙƒØ§ÙØ±ÙŠÙ†"),
        ("Ø§Ù„Ø®Ø§Ø³Ø±ÙŠÙ†", "Ù±Ù„Û¡Ø®ÙÙ°Ø³ÙØ±ÙÙŠÙ†Ù", "Ø§Ù„Ø®Ø§Ø³Ø±ÙŠÙ†"),
        ("Ø§Ù„ØµØ§Ø¨Ø±ÙŠÙ†", "Ù±Ù„ØµÙÙ‘Ù°Ø¨ÙØ±ÙÙŠÙ†Ù", "Ø§Ù„ØµØ§Ø¨Ø±ÙŠÙ†"),
        ("Ø§Ù„ÙƒØ§ÙØ±ÙˆÙ†", "Ù±Ù„Û¡ÙƒÙÙ°ÙÙØ±ÙÙˆÙ†Ù", "Ø§Ù„ÙƒØ§ÙØ±ÙˆÙ†"),
        ("Ø§Ù„Ø®Ø§Ø³Ø±ÙˆÙ†", "Ù±Ù„Û¡Ø®ÙÙ°Ø³ÙØ±ÙÙˆÙ†Ù", "Ø§Ù„Ø®Ø§Ø³Ø±ÙˆÙ†"),
        ("Ø§Ù„ØµØ§Ø¨Ø±ÙˆÙ†", "Ù±Ù„ØµÙÙ‘Ù°Ø¨ÙØ±ÙÙˆÙ†Ù", "Ø§Ù„ØµØ§Ø¨Ø±ÙˆÙ†"),
        ("Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†", "Ù±Ù„Û¡Ù…ÙØ³Û¡Ù„ÙÙ…ÙÙŠÙ†Ù", "Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†"),
        ("Ø§Ù„Ù…Ø¤Ù…Ù†ÙŠÙ†", "Ù±Ù„Û¡Ù…ÙØ¤Û¡Ù…ÙÙ†ÙÙŠÙ†Ù", "Ø§Ù„Ù…ÙˆÙ…Ù†ÙŠÙ†"), # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¤ -> Ùˆ
        ("ÙŠØ§ Ø£ÙŠÙ‡Ø§", "ÙŠÙÙ°Ù“Ø£ÙÙŠÙÙ‘Ù‡ÙØ§", "ÙŠØ§ Ø§ÙŠÙ‡Ø§"), 
        ("ÙŠØ§ Ø¨Ù†ÙŠ", "ÙŠÙÙ°Ø¨ÙÙ†ÙÙ‰Ù“", "ÙŠØ§ Ø¨Ù†ÙŠ"),
        ("Ø§Ù„ÙØ§Ø³Ù‚ÙŠÙ†", "Ù±Ù„Û¡ÙÙÙ°Ø³ÙÙ‚ÙÙŠÙ†Ù", "Ø§Ù„ÙØ§Ø³Ù‚ÙŠÙ†"),
        ("Ø§Ù„ÙØ§Ø³Ù‚ÙˆÙ†", "Ù±Ù„Û¡ÙÙÙ°Ø³ÙÙ‚ÙÙˆÙ†Ù", "Ø§Ù„ÙØ§Ø³Ù‚ÙˆÙ†"),
        ("Ø§Ù„Ù…Ø³Ù„Ù…Ø§Øª", "Ù±Ù„Û¡Ù…ÙØ³Û¡Ù„ÙÙ…ÙÙ°ØªÙ", "Ø§Ù„Ù…Ø³Ù„Ù…Ø§Øª"),
        ("ÙŠØ§ Ø£ÙŠÙ‡Ø§ Ø§Ù„Ø°ÙŠÙ† Ø¢Ù…Ù†ÙˆØ§", "ÙŠÙÙ°Ù“Ø£ÙÙŠÙÙ‘Ù‡ÙØ§ Ù±Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§", "ÙŠØ§ Ø§ÙŠÙ‡Ø§ Ø§Ù„Ø°ÙŠÙ† Ø§Ù…Ù†ÙˆØ§"),
        # ğŸ”¥ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        ("Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…", "Ø¥ÙØ¨Ù’Ø±ÙÙ°Ù‡ÙÛ¦Ù…Ù", "Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…"),
        ("Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†", "Ù±Ù„Ù’Ù…ÙØ³Ù’Ù„ÙÙ…ÙÙŠÙ†Ù", "Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†"),
        ("Ø§Ù„Ù‚Ø§Ù†ØªÙŠÙ†", "Ù±Ù„Ù’Ù‚ÙÙ°Ù†ÙØªÙÙŠÙ†Ù", "Ø§Ù„Ù‚Ø§Ù†ØªÙŠÙ†"),
        ("Ø§Ù„ØµØ§Ø¯Ù‚ÙŠÙ†", "Ù±Ù„ØµÙÙ‘Ù°Ø¯ÙÙ‚ÙÙŠÙ†Ù", "Ø§Ù„ØµØ§Ø¯Ù‚ÙŠÙ†"),
        # Ø­Ø§Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ø£Ø®ÙˆØ°Ø© Ù…Ù† Ù…Ø®Ø±Ø¬Ø§ØªÙƒ
        ("ÙŠØ§ Ø¨Ù†ÙŠ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„", "ÙŠÙÙ°Ø¨ÙÙ†ÙÙ‰Ù“ Ø¥ÙØ³Ù’Ø±ÙÙ°Ù“Ø¡ÙÙŠÙ„Ù", "ÙŠØ§ Ø¨Ù†ÙŠ Ø§Ø³Ø±Ø§ÙŠÙ„"), # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¡ -> Ø§
        ("ÙˆÙØ¢ØªÙÙˆØ§ Ø§Ù„Ø²ÙÙ‘ÙƒÙØ§Ø©Ù", "ÙˆÙØ¡ÙØ§ØªÙÙˆØ§ Ø§Ù„Ø²ÙÙ‘ÙƒÙÙˆÙ°Ø©Ù", "ÙˆØ§ØªÙˆØ§ Ø§Ù„Ø²ÙƒØ§Ù‡"), 
        ("Ù…ÙˆØ³Ù‰", "Ù…ÙÙˆØ³ÙÙ‰", "Ù…ÙˆØ³ÙŠ"), # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ù‰ -> ÙŠ
        ("Ø¹ÙŠØ³Ù‰", "Ø¹ÙÙŠØ³ÙÙ‰", "Ø¹ÙŠØ³ÙŠ"), # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ù‰ -> ÙŠ
        ("ÙŠØ­ÙŠÙ‰", "ÙŠÙØ­Ù’ÙŠÙÙ‰", "ÙŠØ­ÙŠ"), # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ù‰ -> ÙŠ
        ("Ù„ÙƒÙ†", "Ù„ÙÙ°ÙƒÙÙ†ÙÙ‘", "Ù„ÙƒÙ†"),
    ]
    
    total = len(test_cases)
    passed_count = 0
    
    for normal, quranic, expected in test_cases:
        result_normal = normalize_arabic_text(normal)
        result_quranic = normalize_arabic_text(quranic)
        
        is_consistent = (result_normal == result_quranic == expected)
        status = "âœ…" if is_consistent else "âŒ"
        
        if is_consistent:
            passed_count += 1
            
        print(f"{status} Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {normal}")
        
        if not is_consistent:
            print(f"   âŒ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ: {normal} -> {result_normal}")
            print(f"   âŒ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ: {quranic} -> {result_quranic}")
            print(f"   âœ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù€ ({normal}): {expected}")
        
        print("---")

    print("\n" + "="*70)
    print(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {passed_count}/{total} Ù†Ø¬Ø­ | {total - passed_count} ÙØ´Ù„")
    print("="*70 + "\n")
    
    
    # 2. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡
    import time
    def benchmark_normalization(text: str, iterations: int = 5000):
        print(f"â±ï¸  Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ({iterations} ØªÙƒØ±Ø§Ø±)...")
        print(f"   Ø§Ù„Ù†Øµ: {text[:50]}...")
        start = time.time()
        for _ in range(iterations):
            _ = normalize_arabic_text(text)
        elapsed = time.time() - start
        avg_time = (elapsed / iterations) * 1000  # Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
        print(f"   â±ï¸  Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {elapsed:.3f}Ø«")
        print(f"   âš¡ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø²Ù…Ù†: {avg_time:.3f}ms")
        print(f"   ğŸš€ Ø§Ù„Ø³Ø±Ø¹Ø©: {iterations/elapsed:.0f} Ø¹Ù…Ù„ÙŠØ©/Ø«Ø§Ù†ÙŠØ©")

    print("\n" + "="*70)
    print("ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ù†Øµ Ù†Ù…ÙˆØ°Ø¬ÙŠ")
    sample_text = "ÙŠÙØ§ Ø£ÙÙŠÙ‘ÙÙ‡ÙØ§ Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ Ø£ÙÙ‚ÙÙŠÙ…ÙÙˆØ§ Ø§Ù„ØµÙ‘ÙÙ„ÙØ§Ø©Ù ÙˆÙØ¢ØªÙÙˆØ§ Ø§Ù„Ø²Ù‘ÙÙƒÙØ§Ø©Ù"
    benchmark_normalization(sample_text, iterations=5000)
    print("="*70)