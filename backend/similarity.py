"""
ุฏูุงู ุงูุชุดุงุจู ูุงููุนุงูุฌุฉ ุงููุตูุฉ - ูุณุฎุฉ ูุญุณููุฉ ููุฏูุฌุฉ
โ ุฏุนู ุฃูุถู ููุจุญุซ ุงูุนุฑุจู ูุน ุงููุชุงุจุฉ ุงูุนุงุฏูุฉ ูุงูุฑุณู ุงููุฑุขูู
โ ุชูุญูุฏ ุดุงูู ููููุฒุงุช ูุงูุฃุดูุงู ุงููุฎุชููุฉ
โ ุฏุนู ูุงูู ููุฑุณู ุงูุนุซูุงูู
โ ุฅุตูุงุญ ูุดููุฉ ุงูุฌูุน ุงููุฐูุฑ ุงูุณุงูู
โ ุฅุตูุงุญ ูุดููุฉ ุงูุจุญุซ ุจุงูุนุจุงุฑุงุช ุงููุงููุฉ
โ ุฅุตูุงุญ ูุดููุฉ ุงูุชุธููู - "ุขุชูุง ุงูุฒูุงุฉ"
"""

import re
from difflib import SequenceMatcher
from typing import List, Tuple, Dict, Any
import unicodedata

# ============================================
# ๐งน ุชูุธูู ุงููุตูุต ุงูุนุฑุจูุฉ - ูุณุฎุฉ ูุญุณูุฉ ููุฏูุฌุฉ
# ============================================

def normalize_arabic_text(text: str) -> str:
    """
    ุงูุชุทุจูุน ุงูุดุงูู ูููุต ุงูุนุฑุจู - ูุทุงุจู textNormalizer.js ุจุงูุถุจุท
    
    Args:
        text: ุงููุต ุงููุฑุงุฏ ุชุทุจูุนู
        
    Returns:
        ุงููุต ุจุนุฏ ุงูุชุทุจูุน ุงููุงูู
    """
    if not text:
        return ""
    
    normalized = text
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ๐ฅ ุงููุฑุญูุฉ 0: ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ููุนูุงูุงุช ุงูุฎุงุตุฉ (ุฌุฏูุฏุฉ)
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 0.1 ูุนุงูุฌุฉ ุงูููุฒุฉ ุงููุถูููุฉ ูุงูููุฒุฉ ุงููุตู (ูฑ)
    normalized = re.sub(r'ูฑ', 'ุง', normalized)  # ููุฒุฉ ุงููุตู โ ุฃูู
    normalized = re.sub(r'ุฅู', 'ุง', normalized)  # ููุฒุฉ ููุณูุฑุฉ โ ุฃูู
    normalized = re.sub(r'ุฃู', 'ุง', normalized)  # ููุฒุฉ ููุชูุญุฉ โ ุฃูู
    normalized = re.sub(r'ุฃู', 'ุง', normalized)  # ููุฒุฉ ูุถูููุฉ โ ุฃูู
    
    # 0.2 ูุนุงูุฌุฉ ุงูุฃูู ุงูุฎูุฌุฑูุฉ ุงูุดุงููุฉ
    normalized = re.sub(r'([ุจุชุซุฌุญุฎุณุดุตุถุทุธุนุบููููููููู])ููฐ', r'\1ุง', normalized)  # ุญุฑูููฐ โ ุญุฑูุง
    normalized = re.sub(r'([ุจุชุซุฌุญุฎุณุดุตุถุทุธุนุบููููููููู])ููฐ', r'\1ู', normalized)  # ุญุฑูููฐ โ ุญุฑูู
    normalized = re.sub(r'([ุจุชุซุฌุญุฎุณุดุตุถุทุธุนุบููููููููู])ููฐ', r'\1ู', normalized)  # ุญุฑูููฐ โ ุญุฑูู
    
    # 0.3 ูุนุงูุฌุฉ ุงููุงุก ุงูุตุบูุฑุฉ ูุงูุนูุงูุงุช ููู ุงูุญุฑูู
    normalized = re.sub(r'ฆ', '', normalized)  # ูุงุก ุตุบูุฑุฉ (ูู ุฅุจุฑุงููู)
    normalized = re.sub(r'ฅ', '', normalized)  # ูุงู ุตุบูุฑุฉ
    normalized = re.sub(r'ข', '', normalized)  # ุนูุงูุงุช ูุฑุขููุฉ
    normalized = re.sub(r'', '', normalized)
    
    # 0.4 ูุนุงูุฌุฉ ุฎุงุตุฉ ูููููุงุช ุงูุตุนุจุฉ ูู ุณูุฑุฉ ุงูุฃุญุฒุงุจ
    normalized = re.sub(r'ุฅูุจูุฑููฐููฆูู', 'ุงุจุฑุงููู', normalized)
    normalized = re.sub(r'ุฅูุจูุฑููฐููููู', 'ุงุจุฑุงููู', normalized)
    normalized = re.sub(r'ุฅูุจูุฑูุงููููู', 'ุงุจุฑุงููู', normalized)
    normalized = re.sub(r'ุฅูุจูุฑููฐูููู', 'ุงุจุฑุงููู', normalized)
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ุงููุฑุญูุฉ 1: ุฅุฒุงูุฉ ูู ุงูุนูุงูุงุช ูุงูุชุดููู (ุดุงููุฉ 100%)
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 1.1 ุฅุฒุงูุฉ ูู ุงูุญุฑูุงุช ุงูุฃุณุงุณูุฉ
    normalized = re.sub(r'[\u064B-\u065F]', '', normalized)  # ู ู ู ู ู ู ู ู ู ู ู
    
    # 1.2 ุฅุฒุงูุฉ ุนูุงูุงุช ุงููุฏ ูุงูููู ุงููุฑุขููุฉ
    normalized = re.sub(r'[\u0610-\u061A]', '', normalized)  # ุ ุ ุ ุ ุ ุ
    normalized = re.sub(r'[\u06D6-\u06ED]', '', normalized)  #         
    
    # 1.3 ุฅุฒุงูุฉ ุงูุฃูู ุงูุฎูุฌุฑูุฉ ูุนูุงูุงุช ููุณุนุฉ
    normalized = re.sub(r'[\u0670]', '', normalized)         # ูฐ
    normalized = re.sub(r'[\u08D3-\u08E1]', '', normalized)  # ุนูุงูุงุช ูุฑุขููุฉ ููุณุนุฉ
    normalized = re.sub(r'[\u08E3-\u08FF]', '', normalized)  # ุนูุงูุงุช ุฅุถุงููุฉ
    
    # 1.4 ุฅุฒุงูุฉ ุงููุงู ูุงููุงุก ุงูุตุบูุฑุฉ
    normalized = re.sub(r'[ฅฆญ]', '', normalized)
    
    # 1.5 ุฅุฒุงูุฉ Tatweel
    normalized = re.sub(r'[\u0640]', '', normalized)         # ู
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ุงููุฑุญูุฉ 2: ุชูุญูุฏ ุงูููุฒุงุช ูุงูุฃูู (ูุญุณูุฉ)
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 2.1 ุชูุญูุฏ ูู ุฃุดูุงู ุงูุฃูู (ุดุงููุฉ ุฃูุซุฑ)
    normalized = re.sub(r'[ุฃุฅุขูฑุก]', 'ุง', normalized)  # ุฃ ุฅ ุข ูฑ ุก โ ุง
    
    # 2.2 ุงูููุฒุฉ ุนูู ุงููุงู ูุงููุงุก
    normalized = re.sub(r'ุค', 'ู', normalized)
    normalized = re.sub(r'ุฆ', 'ู', normalized)
    normalized = re.sub(r'ูต', 'ุง', normalized)  # ุฃูู ูุน ููุฒุฉ ูุจูุฑุฉ
    normalized = re.sub(r'ูฒ', 'ุง', normalized)  # ุฃูู ูุน ููุฒุฉ ูุงุฆูุฉ
    
    # 2.3 ุงูุฃูู ุงูููุตูุฑุฉ ูุงูุชุงุก ุงููุฑุจูุทุฉ
    normalized = re.sub(r'[ู]', 'ู', normalized)  # ู โ ู
    normalized = re.sub(r'ุฉ', 'ู', normalized)    # ุฉ โ ู
    normalized = re.sub(r'', 'ู', normalized)    # ุชุงุก ูุฑุจูุทุฉ ุจุฏููุฉ
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ๐ฅ ุงููุฑุญูุฉ 3: ูุนุงูุฌุฉ ุงูุฃููุงุท ุงูุนุซูุงููุฉ ุงูุฎุงุตุฉ (ูุญุณูุฉ)
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 3.1 ุงูุฃูู ุงูุฎูุฌุฑูุฉ (ููุท ุนุงู ูุญุณู)
    normalized = re.sub(r'([ุง-ู])ูฐ([ุง-ู])', r'\1ุง\2', normalized)
    normalized = re.sub(r'ูฐ', 'ุง', normalized)
    
    # 3.2 ุงููุงู ุงูุฒุงุฆุฏุฉ ูู ุงููููุงุช ุงูุดุงุฆุนุฉ
    normalized = re.sub(r'([ุต])ููู', r'\1ูุงู', normalized)  # ุตููุฉ โ ุตูุงู
    normalized = re.sub(r'([ุฒ])ููู', r'\1ูุงู', normalized)  # ุฒููุฉ โ ุฒูุงู
    normalized = re.sub(r'([ุฑ])ุจูุง', r'\1ุจุง', normalized)   # ุฑุจูุง โ ุฑุจุง
    normalized = re.sub(r'([ุญ])ููู', r'\1ูุงู', normalized)  # ุญููุฉ โ ุญูุงู
    normalized = re.sub(r'([ู])ููู', r'\1ูุงู', normalized)  # ูููุฉ โ ููุงู
    
    # 3.3 ุงูุฃูู ุงูููุฏูุฏุฉ
    normalized = re.sub(r'([ุง-ู])ุง([ุง-ู])', r'\1ุง\2', normalized)
    
    # 3.4 ุงูุฃุณูุงุก ุงูุฎุงุตุฉ
    normalized = re.sub(r'ุงุจุฑุงููู', 'ุงุจุฑุงููู', normalized)
    normalized = re.sub(r'ุณูููุงู', 'ุณูููุงู', normalized)
    normalized = re.sub(r'ุนูุฑุงู', 'ุนูุฑุงู', normalized)
    
    # 3.5 ุงูููุฒุฉ ุงููุชุญุฑูุฉ ูู ุงูุฃูุนุงู
    normalized = re.sub(r'ุงุงูู', 'ุงูู', normalized)
    normalized = re.sub(r'ุงุงุชูุง', 'ุงุชูุง', normalized)
    normalized = re.sub(r'ุงุงุชู', 'ุงุชู', normalized)
    
    # 3.6 ุงูุฃุฏูุงุช ูุงูุญุฑูู
    normalized = re.sub(r'ุงููุงุฆู', 'ุงููุฆู', normalized)
    normalized = re.sub(r'ุงูููุง', 'ุงููู', normalized)
    normalized = re.sub(r'ูุงุฐุง', 'ูุฐุง', normalized)
    normalized = re.sub(r'ุฐุงูู', 'ุฐูู', normalized)
    
    # 3.7 ุงููุฏุงุก
    normalized = re.sub(r'ูุง ?ุงููุง', 'ูุงุงููุง', normalized)
    normalized = re.sub(r'ูุง ?ุจูู', 'ูุงุจูู', normalized)
    
    # 3.8 "ุงูุฐูู"
    normalized = re.sub(r'ุงูุงุฐูู', 'ุงูุฐูู', normalized)
    normalized = re.sub(r'ุงููุฐูู', 'ุงูุฐูู', normalized)
    
    # 3.9 ๐ฅ ุงููููุงุช ุงูุฎุงุตุฉ ูู ุณูุฑุฉ ุงูุฃุญุฒุงุจ ูุงูุฌููุน (ุฌุฏูุฏุฉ)
    normalized = re.sub(r'ุงููููุณูููููููู', 'ุงููุณูููู', normalized)
    normalized = re.sub(r'ุงููููุณููููููฐุชู', 'ุงููุณููุงุช', normalized)
    normalized = re.sub(r'ุงููููุคูููููููู', 'ุงููุคูููู', normalized)
    normalized = re.sub(r'ุงููููุคููููููฐุชู', 'ุงููุคููุงุช', normalized)
    normalized = re.sub(r'ุงูููููฐููุชูููู', 'ุงููุงูุชูู', normalized)
    normalized = re.sub(r'ุงูููููฐููุชููฐุชู', 'ุงููุงูุชุงุช', normalized)
    normalized = re.sub(r'ุงูุตูููฐุฏูููููู', 'ุงูุตุงุฏููู', normalized)
    normalized = re.sub(r'ุงูุตูููฐุฏููููฐุชู', 'ุงูุตุงุฏูุงุช', normalized)
    normalized = re.sub(r'ุงูุตูููฐุจูุฑูููู', 'ุงูุตุงุจุฑูู', normalized)
    normalized = re.sub(r'ุงูุตูููฐุจูุฑููฐุชู', 'ุงูุตุงุจุฑุงุช', normalized)
    normalized = re.sub(r'ุงููุฎููฐุดูุนูููู', 'ุงูุฎุงุดุนูู', normalized)
    normalized = re.sub(r'ุงููุฎููฐุดูุนููฐุชู', 'ุงูุฎุงุดุนุงุช', normalized)
    normalized = re.sub(r'ุงููููุชูุตูุฏููููููู', 'ุงููุชุตุฏููู', normalized)
    normalized = re.sub(r'ุงููููุชูุตูุฏูููููฐุชู', 'ุงููุชุตุฏูุงุช', normalized)
    normalized = re.sub(r'ุงูุตูููฐูุฆูููููู', 'ุงูุตุงุฆููู', normalized)
    normalized = re.sub(r'ุงูุตูููฐูุฆููููฐุชู', 'ุงูุตุงุฆูุงุช', normalized)
    normalized = re.sub(r'ุงููุญููฐููุธูููู', 'ุงูุญุงูุธูู', normalized)
    normalized = re.sub(r'ุงููุญููฐููุธููฐุชู', 'ุงูุญุงูุธุงุช', normalized)
    normalized = re.sub(r'ุงูุฐูููฐููุฑูููู', 'ุงูุฐุงูุฑูู', normalized)
    normalized = re.sub(r'ุงูุฐูููฐููุฑููฐุชู', 'ุงูุฐุงูุฑุงุช', normalized)
    normalized = re.sub(r'ุงููุฎููฐุณูุฑูููู', 'ุงูุฎุงุณุฑูู', normalized)
    normalized = re.sub(r'ุงููุฎููฐุณูุฑูููู', 'ุงูุฎุงุณุฑูู', normalized)
    normalized = re.sub(r'ุงูููููฐุณูููููู', 'ุงููุงุณููู', normalized)
    normalized = re.sub(r'ุงูููููฐุณูููููู', 'ุงููุงุณููู', normalized)
    normalized = re.sub(r'ุงูุธูููฐููููููู', 'ุงูุธุงูููู', normalized)
    normalized = re.sub(r'ุงูุธูููฐููููููู', 'ุงูุธุงูููู', normalized)
    normalized = re.sub(r'ุงูููููฐููุฑูููู', 'ุงููุงูุฑูู', normalized)
    normalized = re.sub(r'ุงูููููฐููุฑูููู', 'ุงููุงูุฑูู', normalized)
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ุงููุฑุญูุฉ 4: ูุนุงูุฌุฉ ุงูุฌููุน ูุงูุตูุบ ุงููุญููุฉ
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 4.1 ุฌูุน ุงููุฐูุฑ ุงูุณุงูู
    normalized = re.sub(r'ุงู([ุง-ู]{3,})ูู', r'ุงู\1ูู', normalized)
    normalized = re.sub(r'ุงู([ุง-ู]{3,})ูู', r'ุงู\1ูู', normalized)
    
    # 4.2 ุฌูุน ุงููุคูุซ ุงูุณุงูู
    normalized = re.sub(r'ุงู([ุง-ู]{3,})ุงุช', r'ุงู\1ุงุช', normalized)
    
    # 4.3 ุตูุบ ุงูุฃูุนุงู
    normalized = re.sub(r'ูู?ูู', 'ูููู', normalized)
    normalized = re.sub(r'ูู?ุชู', 'ููุชู', normalized)
    normalized = re.sub(r'ูู?ุชู', 'ููุชู', normalized)
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ุงููุฑุญูุฉ 5: ุงูุชูุธูู ุงูููุงุฆู
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # 5.1 ุฅุฒุงูุฉ ุนูุงูุงุช ุงูุชุฑููู
    normalized = re.sub(r'[.,ุุ!?ุ:\-()\[\]{}"\'ยซยป]', '', normalized)
    
    # 5.2 ุฅุฒุงูุฉ ุงูุฃุฑูุงู
    normalized = re.sub(r'[0-9ู-ูฉ]', '', normalized)
    
    # 5.3 ุชูุญูุฏ ุงููุณุงูุงุช
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # 5.4 ุงูุชูุธูู
    normalized = normalized.strip().lower()
    
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    # ๐ฅ ุงููุฑุญูุฉ 6: ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช (ูุญุณูุฉ)
    # โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
    
    # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ุงููุงุชุฌุฉ
    normalized = re.sub(r'ุงุง+', 'ุง', normalized)
    normalized = re.sub(r'ูู+', 'ู', normalized)
    normalized = re.sub(r'ูู+', 'ู', normalized)
    normalized = re.sub(r'ูู+', 'ู', normalized)  # ๐ฅ ุฅุถุงูุฉ ุฌุฏูุฏุฉ
    normalized = re.sub(r'  +', ' ', normalized)
    
    return normalized

# ๐ง ุฏุงูุฉ ุงูุชูุงูู ููููุฏ ุงููุฏูู
def clean_text(text: str) -> str:
    """
    ุฏุงูุฉ ุงูุชูุธูู ุงูุฃุณุงุณูุฉ (ููุชูุงูู ูุน ุงูููุฏ ุงูุญุงูู)
    ุชุณุชุฎุฏู ุงูุชุทุจูุน ุงูุฌุฏูุฏ
    """
    return normalize_arabic_text(text)

# ============================================
# ๐จ ุฏุงูุฉ ุฌุฏูุฏุฉ: ุชุธููู ุงููููุงุช ูู ุงููุต - ูุณุฎุฉ ูุญุณูุฉ
# ============================================

def highlight_words_in_text(text: str, search_query: str) -> str:
    """
    โ ุชุธููู ุฏููู ูููููุงุช ุงููุจุญูุซ ุนููุง ููุท
    
    Args:
        text: ุงููุต ุงูุฃุตูู (ุงูุขูุฉ)
        search_query: ุงูุงุณุชุนูุงู (ูุง ุจุญุซ ุนูู ุงููุณุชุฎุฏู)
        
    Returns:
        ูุต ูุน ุชุธููู ูููููุงุช ุงููุทุงุจูุฉ ููุท
    """
    if not text or not search_query:
        return text
    
    # ุชูุธูู ุงูุงุณุชุนูุงู ููููุงุฑูุฉ
    clean_query = normalize_arabic_text(search_query)
    clean_text_content = normalize_arabic_text(text)
    
    # โ ุงูุญุงูุฉ 1: ุงูุจุญุซ ุนู ุนุจุงุฑุฉ ูุงููุฉ (ุฃูุซุฑ ูู ูููุฉ)
    if ' ' in search_query.strip():
        # ุชูุณูู ุงูุงุณุชุนูุงู ุฅูู ูููุงุช
        query_words = search_query.strip().split()
        
        # ุฅุฒุงูุฉ ุงููููุงุช ุงููุตูุฑุฉ ุฌุฏุงู (ุญุฑู ูุงุญุฏ)
        query_words = [w for w in query_words if len(w) > 1]
        
        if not query_words:
            return text
        
        # โ ูุธูู ูู ูููุฉ ูู ุงูุงุณุชุนูุงู ุจุดูู ุฏููู
        highlighted_text = text
        
        for word in query_words:
            word_clean = normalize_arabic_text(word)
            
            # ุชูุณูู ุงููุต ุฅูู ูููุงุช
            text_words = highlighted_text.split()
            new_words = []
            
            for text_word in text_words:
                # ุชุฌุงูู ุงููููุงุช ุงููุธููุฉ ูุณุจูุงู
                if '<mark>' in text_word or '</mark>' in text_word:
                    new_words.append(text_word)
                    continue
                
                # ุชูุธูู ูููุฉ ุงููุต ููููุงุฑูุฉ
                text_word_clean = normalize_arabic_text(text_word)
                
                # โ ุชุทุงุจู ุฏููู ููุท
                if text_word_clean == word_clean:
                    new_words.append(f'<mark>{text_word}</mark>')
                else:
                    new_words.append(text_word)
            
            highlighted_text = ' '.join(new_words)
        
        return highlighted_text
    
    # โ ุงูุญุงูุฉ 2: ุงูุจุญุซ ุนู ูููุฉ ูุงุญุฏุฉ
    else:
        query_clean = clean_query.strip()
        
        if len(query_clean) < 2:
            return text
        
        # ุชูุณูู ุงููุต ุฅูู ูููุงุช
        words = text.split()
        highlighted_words = []
        
        for word in words:
            # ุชูุธูู ุงููููุฉ ููููุงุฑูุฉ
            word_clean = normalize_arabic_text(word)
            
            # โ ุชุทุงุจู ุฏููู ููุท (ูุณุงูุงุฉ ูุงููุฉ)
            if word_clean == query_clean:
                highlighted_words.append(f'<mark>{word}</mark>')
            else:
                highlighted_words.append(word)
        
        return ' '.join(highlighted_words)
    
# ============================================
# ๐ ุญุณุงุจ ุงูุชุดุงุจู - ูุณุฎุฉ ูุญุณูุฉ
# ============================================

def calculate_similarity(text1: str, text2: str, use_words: bool = True) -> float:
    """
    ุญุณุงุจ ุงูุชุดุงุจู ุจูู ูุตูู ูุน ุงูุชุทุจูุน ุงููุญุณู
    
    Args:
        text1: ุงููุต ุงูุฃูู
        text2: ุงููุต ุงูุซุงูู
        use_words: ุฅุฐุง Trueุ ููุงุฑู ุนูู ูุณุชูู ุงููููุงุช. ุฅุฐุง Falseุ ุนูู ูุณุชูู ุงูุฃุญุฑู
    
    Returns:
        ูุณุจุฉ ุงูุชุดุงุจู (0.0 - 1.0)
    """
    clean1 = normalize_arabic_text(text1)
    clean2 = normalize_arabic_text(text2)
    
    if not clean1 or not clean2:
        return 0.0
    
    if use_words:
        # ุงูููุงุฑูุฉ ุนูู ูุณุชูู ุงููููุงุช (ุฃุฏู)
        words1 = clean1.split()
        words2 = clean2.split()
        return SequenceMatcher(None, words1, words2).ratio()
    else:
        # ุงูููุงุฑูุฉ ุนูู ูุณุชูู ุงูุฃุญุฑู
        return SequenceMatcher(None, clean1, clean2).ratio()

# ============================================
# ๐ง ุฏูุงู ูุณุงุนุฏุฉ ููุจุญุซ ูุงูุฅุญุตุงุฆูุงุช
# ============================================

def normalize_search_query(query: str) -> str:
    """
    ุชุทุจูุน ุงุณุชุนูุงู ุงูุจุญุซ ูุฏุนู ุงููุชุงุจุฉ ุงูุนุงุฏูุฉ ูุงูุฑุณู ุงููุฑุขูู
    """
    return normalize_arabic_text(query)

def get_most_common_words(verses: List[Dict], top_n: int = 5, exclude_common: bool = True) -> List[Dict]:
    """
    ุงุณุชุฎุฑุงุฌ ุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู ูู ุงูุขูุงุช
    
    Args:
        verses: ูุงุฆูุฉ ุงูุขูุงุช
        top_n: ุนุฏุฏ ุงููููุงุช ุงููุทููุจุฉ
        exclude_common: ุงุณุชุจุนุงุฏ ุงููููุงุช ุงูุดุงุฆุนุฉ (ุญุฑูู ุงูุฌุฑ etc)
    
    Returns:
        ูุงุฆูุฉ ุงููููุงุช ุงูุฃูุซุฑ ุชูุฑุงุฑุงู
    """
    # ูููุงุช ุดุงุฆุนุฉ ูุฌุจ ุงุณุชุจุนุงุฏูุง
    common_words = {
        'ูู', 'ูู', 'ุฅูู', 'ุนูู', 'ุนู', 'ุฃู', 'ุฅู', 'ูุง', 'ูุง', 'ูู', 'ุจู',
        'ูุฏ', 'ุณู', 'ูุงู', 'ูููู', 'ูุงู', 'ูู', 'ุฅู', 'ุฃู', 'ูู', 'ูู', 'ูู',
        'ูุฐูู', 'ุงูุฐู', 'ุงูุชู', 'ุงูุฐูู', 'ุงููุงุชู', 'ุงููุงุฆู', 'ุฐูู', 'ูุฐู',
        'ูุฐุง', 'ูุคูุงุก', 'ุชูู', 'ุฃููุฆู', 'ุจุนุถ', 'ูู', 'ุฌููุน', 'ุฃู', 'ุฃูู',
        'ูุชู', 'ููู', 'ููุงุฐุง', 'ูู', 'ุฃูุถุง', 'ุซู', 'ุญุชู', 'ุฃูุง', 'ุฃู', 'ู'
    }
    
    word_count = {}
    
    for verse in verses:
        text = normalize_arabic_text(verse.get('text', ''))
        words = text.split()
        
        for word in words:
            if len(word) < 2:  # ุชุฌุงูู ุงูุฃุญุฑู ุงููููุฑุฏุฉ
                continue
                
            if exclude_common and word in common_words:
                continue
                
            word_count[word] = word_count.get(word, 0) + 1
    
    # ุชุฑุชูุจ ุงููููุงุช ุญุณุจ ุงูุชูุฑุงุฑ
    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
    
    return [{'word': word, 'count': count} for word, count in sorted_words[:top_n]]

def get_most_common_phrases(verses: List[Dict], top_n: int = 5, phrase_length: int = 3) -> List[Dict]:
    """
    ุงุณุชุฎุฑุงุฌ ุฃูุซุฑ ุงูุนุจุงุฑุงุช ุชูุฑุงุฑุงู ูู ุงูุขูุงุช
    
    Args:
        verses: ูุงุฆูุฉ ุงูุขูุงุช
        top_n: ุนุฏุฏ ุงูุนุจุงุฑุงุช ุงููุทููุจุฉ
        phrase_length: ุทูู ุงูุนุจุงุฑุฉ (ุนุฏุฏ ุงููููุงุช)
    
    Returns:
        ูุงุฆูุฉ ุงูุนุจุงุฑุงุช ุงูุฃูุซุฑ ุชูุฑุงุฑุงู
    """
    phrase_count = {}
    
    for verse in verses:
        text = normalize_arabic_text(verse.get('text', ''))
        words = text.split()
        
        # ุงุณุชุฎุฑุงุฌ ุงูุนุจุงุฑุงุช
        for i in range(len(words) - phrase_length + 1):
            phrase = ' '.join(words[i:i + phrase_length])
            
            if len(phrase.strip()) < phrase_length * 2:  # ุชุฌุงูู ุงูุนุจุงุฑุงุช ุงููุตูุฑุฉ ุฌุฏุงู
                continue
                
            phrase_count[phrase] = phrase_count.get(phrase, 0) + 1
    
    # ุชุฑุชูุจ ุงูุนุจุงุฑุงุช ุญุณุจ ุงูุชูุฑุงุฑ
    sorted_phrases = sorted(phrase_count.items(), key=lambda x: x[1], reverse=True)
    
    return [{'phrase': phrase, 'count': count} for phrase, count in sorted_phrases[:top_n]]

# ============================================
# ๐จ ุชูููุฒ ุงููุฑููุงุช (ููุณ ุงูุฏูุงู ุงูุณุงุจูุฉ)
# ============================================

def highlight_differences(text1: str, text2: str) -> Tuple[List[dict], List[dict]]:
    """
    ุชูููุฒ ุงููุฑููุงุช ุจูู ูุตูู
    """
    clean1 = normalize_arabic_text(text1)
    clean2 = normalize_arabic_text(text2)
    
    words1 = clean1.split()
    words2 = clean2.split()
    
    matcher = SequenceMatcher(None, words1, words2)
    
    highlighted1 = []
    highlighted2 = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            for i in range(i1, i2):
                highlighted1.append({'type': 'same', 'text': words1[i]})
            for j in range(j1, j2):
                highlighted2.append({'type': 'same', 'text': words2[j]})
        
        elif tag == 'replace':
            for i in range(i1, i2):
                highlighted1.append({'type': 'diff', 'text': words1[i]})
            for j in range(j1, j2):
                highlighted2.append({'type': 'diff', 'text': words2[j]})
        
        elif tag == 'delete':
            for i in range(i1, i2):
                highlighted1.append({'type': 'diff', 'text': words1[i]})
        
        elif tag == 'insert':
            for j in range(j1, j2):
                highlighted2.append({'type': 'diff', 'text': words2[j]})
    
    return highlighted1, highlighted2

# ============================================
# ๐งช ุงุฎุชุจุงุฑ ุงูุฏูุงู ุงููุญุณูุฉ
# ============================================

if __name__ == "__main__":
    print("๐งช ุงุฎุชุจุงุฑ ุงูุชุทุจูุน ุงููุญุณู:")
    
    test_cases = [
        # (ุงููุชุงุจุฉ ุงูุนุงุฏูุฉ, ุงูุฑุณู ุงููุฑุขูู, ุงููุชููุน ุจุนุฏ ุงูุชุทุจูุน)
        ("ุงูุตูุงุฉ", "ูฑูุตููููููฐุฉู", "ุงูุตูุงู"),
        ("ุงูุฒูุงุฉ", "ูฑูุฒููููููฐุฉู", "ุงูุฒูุงู"),
        ("ูุขุชูุง", "ููุกูุงุชููุงู", "ูุงุชูุง"),
        ("ุงูุฑุจุง", "ูฑูุฑููุจูููฐุงู", "ุงูุฑุจุง"),
        ("ุงูุธุงูููู", "ูฑูุธูููฐููููููู", "ุงูุธุงูููู"),
        ("ุงููุงูุฑูู", "ูฑูกูููฐููุฑูููู", "ุงููุงูุฑูู"),
        ("ุงูุฎุงุณุฑูู", "ูฑูกุฎููฐุณูุฑูููู", "ุงูุฎุงุณุฑูู"),
        ("ุงูุตุงุจุฑูู", "ูฑูุตูููฐุจูุฑูููู", "ุงูุตุงุจุฑูู"),
        ("ุงููุงูุฑูู", "ูฑูกูููฐููุฑูููู", "ุงููุงูุฑูู"),
        ("ุงูุฎุงุณุฑูู", "ูฑูกุฎููฐุณูุฑูููู", "ุงูุฎุงุณุฑูู"),
        ("ุงูุตุงุจุฑูู", "ูฑูุตูููฐุจูุฑูููู", "ุงูุตุงุจุฑูู"),
        ("ุงููุณูููู", "ูฑูกููุณกููููููู", "ุงููุณูููู"),
        ("ุงููุคูููู", "ูฑูกููุคกููููููู", "ุงููุคูููู"),
        ("ูุง ุฃููุง", "ูููฐูุฃููููููุง", "ูุง ุงููุง"),
        ("ูุง ุจูู", "ูููฐุจููููู", "ูุง ุจูู"),
        ("ุงููุงุณููู", "ูฑูกูููฐุณูููููู", "ุงููุงุณููู"),
        ("ุงููุงุณููู", "ูฑูกูููฐุณูููููู", "ุงููุงุณููู"),
        ("ุงููุณููุงุช", "ูฑูกููุณกูููููฐุชู", "ุงููุณููุงุช"),
        ("ูุง ุฃููุง ุงูุฐูู ุขูููุง", "ูููฐูุฃููููููุง ูฑูููุฐูููู ุขูููููุง", "ูุง ุงููุง ุงูุฐูู ุงูููุง"),
        # ๐ฅ ุงูุงุฎุชุจุงุฑุงุช ุงูุฌุฏูุฏุฉ
        ("ุงุจุฑุงููู", "ุฅูุจูุฑููฐููฆูู", "ุงุจุฑุงููู"),
        ("ุงููุณูููู", "ูฑููููุณูููููููู", "ุงููุณูููู"),
        ("ุงููุงูุชูู", "ูฑูููููฐููุชูููู", "ุงููุงูุชูู"),
        ("ุงูุตุงุฏููู", "ูฑูุตูููฐุฏูููููู", "ุงูุตุงุฏููู")
    ]
    
    for normal, quranic, expected in test_cases:
        result_normal = normalize_arabic_text(normal)
        result_quranic = normalize_arabic_text(quranic)
        
        print(f"๐ ุงูุนุงุฏู: {normal}")
        print(f"๐ ุงููุฑุขูู: {quranic}")
        print(f"๐ ุงููุชูุฌุฉ: {result_normal} | {result_quranic}")
        print(f"โ ูุชุทุงุจูุฉ: {result_normal == result_quranic == expected}")
        print("---")