"""
Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ - Ù†Ø³Ø®Ø© Ù‡Ø¬ÙŠÙ†Ø© Ø°ÙƒÙŠØ© (Smart Hybrid V5.3) + ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ†
âœ… FAISS Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† (Ø³Ø±ÙŠØ¹)
âœ… Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ø¯Ù‚ÙŠÙ‚)
âœ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ 100% Ù…Ù† Quiz
âœ… Ø­Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªÙØ¹ (10000)
âœ… Ù†Ø³Ø¨Ø© ØªØ´Ø§Ø¨Ù‡ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
âœ… Ø§Ø®ØªØ¨Ø§Ø± "Ù…Ø§ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø©ØŸ"
âœ… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†
âœ… Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
âœ… ØªØ¸Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
âœ… ğŸ† ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± (Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙƒÙ„Ù…Ø©)

ğŸš€ Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª):
âœ… Ø¨Ø­Ø« ÙÙˆØ±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FTS5 (5-20ms)
âœ… Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙÙˆØ±ÙŠØ© Ù…Ù† Cache (10-50ms) 
âœ… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Cache
âœ… AutoComplete Ù„Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
âœ… Ù†Ø¸Ø§Ù… Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø§Ø±Ø³ ØªÙ„Ù‚Ø§Ø¦ÙŠ
âœ… ğŸš€ Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ Ù…Ø³Ø±Ù‘Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache
âœ… ğŸ” Ø¨Ø­Ø« Ù†ØµÙŠ Ù…Ø­Ø³Ù‘Ù† ÙˆØ³Ø±ÙŠØ¹
"""

import random
from fastapi import FastAPI, Depends, HTTPException, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import func, or_
from database import get_db, Verse, init_db
from typing import List, Optional
import time
from difflib import SequenceMatcher
from contextlib import asynccontextmanager
import numpy as np
import faiss
#from sentence_transformers import SentenceTransformer
import json
from pathlib import Path as FilePath
import sqlite3
from functools import lru_cache
import os

# ============================================
# âš™ï¸ Production Configuration
# ============================================
"""
Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Production - ØªØ¯Ø¹Ù… Render.com Ùˆ Railway Ùˆ Heroku
"""

# 1ï¸âƒ£ Port Configuration
PORT = int(os.environ.get("PORT", 8000))
print(f"ğŸŒ Port: {PORT}")

# 2ï¸âƒ£ Host Configuration
# Render.com ÙŠØ­ØªØ§Ø¬ 0.0.0.0 (Ù„ÙŠØ³ 127.0.0.1)
HOST = os.environ.get("HOST", "0.0.0.0")
print(f"ğŸŒ Host: {HOST}")

# 3ï¸âƒ£ Production Mode
PRODUCTION = os.environ.get("PRODUCTION", "false").lower() == "true"
print(f"ğŸš€ Production Mode: {PRODUCTION}")

# 4ï¸âƒ£ Database URL (Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ - Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª PostgreSQL)
DATABASE_URL = os.environ.get("DATABASE_URL", "sqlite:///./quran.db")
print(f"ğŸ’¾ Database: {DATABASE_URL}")

# 5ï¸âƒ£ CORS Origins (Ù„Ù„Ø£Ù…Ø§Ù†)
ALLOWED_ORIGINS = os.environ.get(
    "ALLOWED_ORIGINS", 
    "http://localhost:5173,http://127.0.0.1:5173"
).split(",")
print(f"ğŸ” Allowed Origins: {ALLOWED_ORIGINS}")

# 6ï¸âƒ£ Workers (Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡)
WORKERS = int(os.environ.get("WORKERS", 1))
print(f"âš¡ Workers: {WORKERS}")


# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
from similarity import normalize_arabic_text as clean_text, highlight_differences, calculate_similarity, highlight_words_in_text


# ============================================
# âŒ ØªØ¹Ø·ÙŠÙ„ Ù†Ø¸Ø§Ù… embeddings ÙÙŠ Production
# ============================================
EMBEDDING_AVAILABLE = False
print("âš ï¸ Ù†Ø¸Ø§Ù… embeddings Ù…Ø¹Ø·Ù„ ÙÙŠ Production - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ ÙÙ‚Ø·")

# ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
QURAN_EMBEDDINGS = None
QURAN_IDS = None  
FAISS_INDEX = None
EMBEDDING_MODEL = None

# ============================================
# ğŸš« Ù‚Ø§Ø¦Ù…Ø© Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ù„Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª 100% (Ù„Ø§ ØªÙØ¸Ù‡Ø±)
# ============================================

EXCLUDED_100_PERCENT_PATTERNS = {
    # Ø§Ù„Ø¨Ø³Ù…Ù„Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø£Ø´ÙƒØ§Ù„Ù‡Ø§
    "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…",
    "Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù",
    "Ø¨Ù‘ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù",
    
    # Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© ÙƒØ«ÙŠØ±Ø§Ù‹
    "ÙÙØ¨ÙØ£ÙÙŠÙ‘Ù Ø¢Ù„ÙØ§Ø¡Ù Ø±ÙØ¨Ù‘ÙÙƒÙÙ…ÙØ§ ØªÙÙƒÙØ°Ù‘ÙØ¨ÙØ§Ù†Ù",
    "ÙˆÙÙŠÙ’Ù„ÙŒ ÙŠÙÙˆÙ’Ù…ÙØ¦ÙØ°Ù Ù„Ù‘ÙÙ„Ù’Ù…ÙÙƒÙØ°Ù‘ÙØ¨ÙÙŠÙ†Ù", 
    #"Ø¥ÙÙ†Ù‘Ù ÙÙÙŠ Ø°ÙÙ„ÙÙƒÙ Ù„ÙØ¢ÙŠÙØ©Ù‹ ÙˆÙÙ…ÙØ§ ÙƒÙØ§Ù†Ù Ø£ÙÙƒÙ’Ø«ÙØ±ÙÙ‡ÙÙ… Ù…Ù‘ÙØ¤Ù’Ù…ÙÙ†ÙÙŠÙ†Ù",
    
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù‡Ù†Ø§
    #"Ø¥ÙÙ†Ù‘Ù Ø±ÙØ¨Ù‘ÙÙƒÙ Ø­ÙÙƒÙÙŠÙ…ÙŒ Ø¹ÙÙ„ÙÙŠÙ…ÙŒ",
    #"Ø¥ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù ØºÙÙÙÙˆØ±ÙŒ Ø±Ù‘ÙØ­ÙÙŠÙ…ÙŒ",
    #"ÙˆÙØ§Ù„Ù„Ù‘ÙÙ‡Ù Ø¹ÙÙ„ÙÙŠÙ…ÙŒ Ø­ÙÙƒÙÙŠÙ…ÙŒ"
}

def is_excluded_100_percent_match(text1: str, text2: str) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¢ÙŠØªÙŠÙ† Ù…ØªØ·Ø§Ø¨Ù‚ØªÙŠÙ† 100% ÙˆÙ…Ø³ØªØ«Ù†ÙŠØªÙŠÙ† Ù…Ù† Ø§Ù„Ø¹Ø±Ø¶
    """
    # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙˆÙ†Ø§ Ù…ØªØ·Ø§Ø¨Ù‚ØªÙŠÙ† 100%ØŒ Ù„Ø§ ØªØ³ØªØ«Ù†ÙŠÙ‡Ù…Ø§
    if calculate_word_similarity(text1, text2) < 0.99:
        return False
    
    clean1 = clean_text(text1)
    clean2 = clean_text(text2)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØµÙŠÙ† Ù…ØªØ·Ø§Ø¨Ù‚ÙŠÙ† ÙØ¹Ù„Ø§Ù‹
    if clean1 != clean2:
        return False
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¢ÙŠØ© ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª
    for pattern in EXCLUDED_100_PERCENT_PATTERNS:
        pattern_clean = clean_text(pattern)
        if pattern_clean in clean1 or pattern_clean in clean2:
            return True
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨Ø³Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ
    if is_basmala_text(text1) or is_basmala_text(text2):
        return True
        
    return False

def is_basmala_text(text: str) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù‡Ùˆ Ø§Ù„Ø¨Ø³Ù…Ù„Ø©"""
    basmala_variations = [
        "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…",
        "Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù", 
        "Ø¨Ù‘ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù"
    ]
    text_clean = clean_text(text)
    return any(clean_text(basmala) in text_clean for basmala in basmala_variations)

# ============================================
# ğŸŒŸ Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
# ============================================
QURAN_EMBEDDINGS: Optional[np.ndarray] = None
QURAN_IDS: Optional[np.ndarray] = None
FAISS_INDEX: Optional[faiss.Index] = None
EMBEDDING_MODEL: Optional[any] = None

# ============================================
# ğŸ† Ù…ØªØºÙŠØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±
# ============================================
MUTASHABIHAT_BANK = None

# ============================================
# ğŸš€ Ù…ØªØºÙŠØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
# ============================================
SIMILARITY_CACHE = None
WORD_STATS_CACHE = None
FTS_AVAILABLE = False

# ============================================
# âœ… Ø¯Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ (Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª)
# ============================================
def calculate_word_similarity(text1: str, text2: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    return calculate_similarity(text1, text2, use_words=True)

# ============================================
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ============================================
def is_basmala_verse(verse: Verse) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø¢ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø¨Ø³Ù…Ù„Ø© - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©"""
    if verse.ayah != 1 or verse.surah == 9:  # Ø³ÙˆØ±Ø© Ø§Ù„ØªÙˆØ¨Ø© Ù„Ø§ ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¨Ø³Ù…Ù„Ø©
        return False
    
    verse_clean = clean_text(verse.text)
    basmala_variations = [
        clean_text("Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…"),
        clean_text("Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…"),
        clean_text("Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù"),
        clean_text("Ø¨Ù‘ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù"),
        "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…",  # Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ
    ]
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø´ÙƒØ§Ù„
    for basmala in basmala_variations:
        if (verse_clean == basmala or 
            verse_clean.startswith(basmala) or
            basmala in verse_clean):
            return True
    
    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨Ø³Ù…Ù„Ø©
    return len(verse_clean) < 30 and any(word in verse_clean for word in ['Ø¨Ø³Ù…', 'Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ø±Ø­Ù…Ù†', 'Ø§Ù„Ø±Ø­ÙŠÙ…'])

def initialize_search_engine(db: Session):
    """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (FAISS) - Ù…Ø¹Ø·Ù„ ÙÙŠ Production"""
    global QURAN_EMBEDDINGS, QURAN_IDS, FAISS_INDEX, EMBEDDING_MODEL
    
    print("\n" + "="*60)
    print("ğŸš« Ù†Ø¸Ø§Ù… FAISS Ù…Ø¹Ø·Ù„ ÙÙŠ Production - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ ÙÙ‚Ø·")
    print("="*60 + "\n")
    
    # ØªØ¹Ø·ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…ØªØºÙŠØ±Ø§Øª FAISS ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    QURAN_EMBEDDINGS = None
    QURAN_IDS = None
    FAISS_INDEX = None
    EMBEDDING_MODEL = None
    
    print("âœ… ØªÙ… ØªØ¹Ø·ÙŠÙ„ Ù†Ø¸Ø§Ù… FAISS ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
    print("ğŸ’¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø³ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ ÙÙ‚Ø· (Ø£Ø³Ø±Ø¹ ÙˆØ£Ø®Ù)")
    print("   â€¢ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ â†’ Ø¯Ù‚ÙŠÙ‚ 100% âœ“")
    print("   â€¢ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ â†’ Ù†ØªØ§Ø¦Ø¬ Ù…Ø¶Ù…ÙˆÙ†Ø© âœ“")
    print("   â€¢ FTS5 â†’ Ø¨Ø­Ø« ÙÙˆØ±ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø© âš¡")
    print("   â€¢ Cache â†’ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙÙˆØ±ÙŠØ© ğŸš€\n")

# ============================================
# ğŸš€ Ø¯ÙˆØ§Ù„ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
# ============================================

def initialize_optimizations(db: Session):
    """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
    global SIMILARITY_CACHE, WORD_STATS_CACHE, FTS_AVAILABLE
    
    print("\n" + "="*60)
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª")
    print("="*60)
    
    # 1. ØªØ­Ù…ÙŠÙ„ similarity cache
    try:
        if os.path.exists("similarity_cache.npy"):
            SIMILARITY_CACHE = np.load("similarity_cache.npy", allow_pickle=True).item()
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ similarity cache: {len(SIMILARITY_CACHE)} Ø¢ÙŠØ©")
        else:
            SIMILARITY_CACHE = {}
            print("âš ï¸ similarity cache ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ similarity cache: {e}")
        SIMILARITY_CACHE = {}
    
    # 2. ØªØ­Ù…ÙŠÙ„ word statistics cache
    try:
        if os.path.exists("word_stats_cache.json"):
            with open("word_stats_cache.json", 'r', encoding='utf-8') as f:
                WORD_STATS_CACHE = json.load(f)
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ word stats cache: {len(WORD_STATS_CACHE)} ÙƒÙ„Ù…Ø©")
        else:
            WORD_STATS_CACHE = {}
            print("âš ï¸ word stats cache ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ word stats cache: {e}")
        WORD_STATS_CACHE = {}
    
    # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† FTS5 ÙˆØ¥ØµÙ„Ø§Ø­Ù‡ Ø¥Ø°Ø§ Ù„Ø²Ù…
    try:
        conn = sqlite3.connect('quran.db')
        cursor = conn.cursor()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ FTS5
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='verses_fts'")
        fts_table_exists = cursor.fetchone() is not None
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ FTS5
        if fts_table_exists:
            cursor.execute("SELECT COUNT(*) FROM verses_fts")
            fts_count = cursor.fetchone()[0]
            
            if fts_count == 6236:  # Ù†ÙØ³ Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª
                FTS_AVAILABLE = True
                print("âœ… FTS5 index Ù…ØªØ§Ø­ ÙˆÙ…ÙƒØªÙ…Ù„ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„ÙÙˆØ±ÙŠ")
            else:
                print(f"âš ï¸ FTS5 ØºÙŠØ± Ù…ÙƒØªÙ…Ù„ ({fts_count}/6236)ØŒ Ø¬Ø§Ø±ÙŠ Ø¥ØµÙ„Ø§Ø­Ù‡...")
                rebuild_fts_for_arabic(db)
        else:
            print("âš ï¸ Ø¬Ø¯ÙˆÙ„ FTS5 ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡...")
            rebuild_fts_for_arabic(db)
            
        conn.close()
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† FTS5: {e}")
        FTS_AVAILABLE = False
    
    print("âœ… Ø§ÙƒØªÙ…Ù„Øª ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª\n")

def fast_text_search_fts(query: str, limit: int = 20):
    """
    ğŸ”¥ FTS5 Ù…Ø­Ø³Ù† - Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
    """
    if not FTS_AVAILABLE:
        return []
    
    try:
        conn = sqlite3.connect('quran.db')
        cursor = conn.cursor()
        
        # âœ… Ø¥ØµÙ„Ø§Ø­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø­Ø« FTS5 Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        # FTS5 ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø© ÙƒØ§Ù…Ù„Ø© Ø£Ùˆ Ø¬Ø²Ø¡ Ù…Ù†Ù‡Ø§
        fts_query = f'"{query}" OR "{query}"*'
        
        # ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¨Ø£ÙƒØ«Ø± Ù…Ù† Ø·Ø±ÙŠÙ‚Ø©
        queries_to_try = [
            f'"{query}"',          # Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©
            f'"{query}"*',         # ØªØ¨Ø¯Ø£ Ø¨Ù€
            f'*"{query}"*',        # ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ø§Ù‹)
            clean_text(query)      # Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ
        ]
        
        results = []
        seen_ids = set()
        
        for fts_q in queries_to_try:
            if len(results) >= limit:
                break
                
            try:
                cursor.execute(f'''
                    SELECT verses.* 
                    FROM verses_fts
                    JOIN verses ON verses_fts.rowid = verses.id
                    WHERE verses_fts.text MATCH ?
                    ORDER BY rank
                    LIMIT ?
                ''', (fts_q, limit * 2))
                
                for row in cursor.fetchall():
                    if row[0] in seen_ids:
                        continue
                        
                    seen_ids.add(row[0])
                    results.append({
                        'id': row[0],
                        'surah': row[1],
                        'surah_name': row[2],
                        'ayah': row[3],
                        'text': row[4],
                        'juz': row[5],
                        'similarity': '0.9500',
                        'match_type': 'fts_fast',
                        'note': 'Ù†ØªÙŠØ¬Ø© Ø³Ø±ÙŠØ¹Ø©'
                    })
                    
                    if len(results) >= limit:
                        break
                        
            except:
                continue
        
        conn.close()
        
        if results:
            print(f"âœ… FTS5: {len(results)} Ù†ØªÙŠØ¬Ø© Ù„Ù„Ø¨Ø­Ø« '{query}'")
            
        return results
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« FTS5: {e}")
        return []
        
@lru_cache(maxsize=1000)
def get_cached_similarities(verse_id: int, min_similarity: float = 0.6):
    """
    ğŸš€ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙÙˆØ±ÙŠØ© Ù…Ù† Cache
    ğŸš€ Ø§Ù„Ø³Ø±Ø¹Ø©: 10-50ms Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø®Ø²Ù†Ø©
    """
    global SIMILARITY_CACHE
    
    if SIMILARITY_CACHE and verse_id in SIMILARITY_CACHE:
        cached_results = SIMILARITY_CACHE[verse_id]
        # ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
        filtered = [r for r in cached_results if r['similarity'] >= min_similarity]
        return filtered[:20]  # Ø¥Ø±Ø¬Ø§Ø¹ 20 Ù†ØªÙŠØ¬Ø© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
    
    return []  # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ÙÙŠ cacheØŒ Ù†Ø±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©

def build_similarity_cache(db: Session, min_similarity: float = 0.05):  # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: 0.05 Ø¨Ø¯Ù„ 0.6
    """
    Ø¨Ù†Ø§Ø¡ similarity cache Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¢ÙŠØ§Øª - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
    âš ï¸ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ - ÙŠÙØ´ØºÙ‘Ù„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
    """
    global SIMILARITY_CACHE
    
    print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ similarity cache...")
    print(f"   ğŸ¯ Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {min_similarity*100}%")
    start_time = time.time()
    
    all_verses = db.query(Verse).all()
    total_verses = len(all_verses)
    SIMILARITY_CACHE = {}
    
    for i, verse in enumerate(all_verses):
        if (i + 1) % 100 == 0:
            elapsed = time.time() - start_time
            print(f"   ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {i+1}/{total_verses} Ø¢ÙŠØ© ({elapsed:.1f}Ø«)")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©
        similar_verses = []
        
        # âœ… Ø¨Ø­Ø« Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª 100%
        for other_verse in all_verses:
            if other_verse.id == verse.id:
                continue
                
            similarity = calculate_word_similarity(verse.text, other_verse.text)
            
            if similarity >= min_similarity and similarity < 0.99:
                # ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
                existing = any(sv['verse_id'] == other_verse.id for sv in similar_verses)
                if not existing:
                    similar_verses.append({
                        'verse_id': other_verse.id,
                        'surah': other_verse.surah,
                        'surah_name': other_verse.surah_name,
                        'ayah': other_verse.ayah,
                        'text': other_verse.text,
                        'similarity': similarity
                    })

        # âœ… ØªØ®Ø²ÙŠÙ† 50 Ù†ØªÙŠØ¬Ø© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰ (ÙƒØ§Ù† 20)
        SIMILARITY_CACHE[verse.id] = similar_verses[:50]
        
    # Ø­ÙØ¸ cache ÙÙŠ Ù…Ù„Ù
    try:
        np.save("similarity_cache.npy", SIMILARITY_CACHE)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ similarity cache: {len(SIMILARITY_CACHE)} Ø¢ÙŠØ©")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ similarity cache: {e}")
    
    elapsed = time.time() - start_time
    print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ similarity cache ÙÙŠ {elapsed:.1f} Ø«Ø§Ù†ÙŠØ©")
    
    return SIMILARITY_CACHE

def build_word_statistics_cache(db: Session):
    """
    Ø¨Ù†Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ø³Ø¨Ù‚Ø§Ù‹ - Ù…ÙØ­Ø¯Ù‘Ø«
    """
    global WORD_STATS_CACHE
    
    print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ word statistics cache...")
    start_time = time.time()
    
    all_verses = db.query(Verse).all()
    WORD_STATS_CACHE = {}
    
    # ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§
    common_words = {
        'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ø£Ù†', 'Ø¥Ù†', 'Ù…Ø§', 'Ù„Ø§', 'Ù‡Ù„', 'Ø¨Ù„',
        'Ù‚Ø¯', 'Ø³Ù‰', 'ÙƒØ§Ù†', 'ÙŠÙƒÙˆÙ†', 'Ù‚Ø§Ù„', 'Ù‚Ù„', 'Ø¥Ù†', 'Ø£Ù†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ù‡Ù…',
        'ÙƒØ°Ù„Ùƒ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø°Ù„Ùƒ', 'Ù‡Ø°Ù‡',
        'Ù‡Ø°Ø§', 'Ù‡Ø¤Ù„Ø§Ø¡', 'ØªÙ„Ùƒ', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø¨Ø¹Ø¶', 'ÙƒÙ„', 'Ø¬Ù…ÙŠØ¹', 'Ø£ÙŠ', 'Ø£ÙŠÙ†',
        'Ù…ØªÙ‰', 'ÙƒÙŠÙ', 'Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙ…', 'Ø£ÙŠØ¶Ø§', 'Ø«Ù…', 'Ø­ØªÙ‰', 'Ø£Ù…Ø§', 'Ø£Ùˆ', 'Ùˆ'
    }
    
    for verse in all_verses:
        verse_clean = clean_text(verse.text)
        words = verse_clean.split()
        
        for word in words:
            if len(word) < 2 or word in common_words:
                continue
                
            if word not in WORD_STATS_CACHE:
                WORD_STATS_CACHE[word] = {
                    'total_count': 0,
                    'verses_count': 0,
                    'verses': [],  # âœ… Ø³ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {verse_info, count}
                    'by_surah': {},
                    'by_juz': {}
                }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©
            count_in_verse = verse_clean.count(word)
            WORD_STATS_CACHE[word]['total_count'] += count_in_verse
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¢ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
            verse_info = {
                'id': verse.id,
                'surah': verse.surah,
                'surah_name': verse.surah_name,
                'ayah': verse.ayah,
                'text': verse.text,
                'juz': verse.juz,
                'count': count_in_verse  # âœ… Ø¥Ø¶Ø§ÙØ© count Ù‡Ù†Ø§ Ø£ÙŠØ¶Ø§Ù‹
            }
            
            # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            existing_verse = next((v for v in WORD_STATS_CACHE[word]['verses'] 
                                if v['id'] == verse.id), None)
            if not existing_verse:
                WORD_STATS_CACHE[word]['verses'].append(verse_info)
                WORD_STATS_CACHE[word]['verses_count'] = len(WORD_STATS_CACHE[word]['verses'])
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø³ÙˆØ±Ø©
            surah_key = f"{verse.surah_name} ({verse.surah})"
            WORD_STATS_CACHE[word]['by_surah'][surah_key] = WORD_STATS_CACHE[word]['by_surah'].get(surah_key, 0) + count_in_verse
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø²Ø¡
            if verse.juz:
                juz_key = f"Ø§Ù„Ø¬Ø²Ø¡ {verse.juz}"
                WORD_STATS_CACHE[word]['by_juz'][juz_key] = WORD_STATS_CACHE[word]['by_juz'].get(juz_key, 0) + count_in_verse
    
    # Ø­ÙØ¸ cache ÙÙŠ Ù…Ù„Ù
    try:
        with open("word_stats_cache.json", 'w', encoding='utf-8') as f:
            json.dump(WORD_STATS_CACHE, f, ensure_ascii=False, indent=2)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ word stats cache: {len(WORD_STATS_CACHE)} ÙƒÙ„Ù…Ø©")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ word stats cache: {e}")
    
    elapsed = time.time() - start_time
    print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ word statistics cache ÙÙŠ {elapsed:.1f} Ø«Ø§Ù†ÙŠØ©")
    
    return WORD_STATS_CACHE

def build_fts_index(db: Session):
    """
    Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5 Ù„Ù„Ø¨Ø­Ø« Ø§Ù„ÙÙˆØ±ÙŠ
    """
    print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5...")
    start_time = time.time()
    
    try:
        conn = sqlite3.connect('quran.db')
        cursor = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ FTS5
        cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS verses_fts 
            USING fts5(text, content=verses, content_rowid=id)
        ''')
        
        # Ù…Ù„Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ (Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºØ§Ù‹)
        cursor.execute('SELECT COUNT(*) FROM verses_fts')
        count = cursor.fetchone()[0]
        
        if count == 0:
            cursor.execute('''
                INSERT INTO verses_fts(rowid, text)
                SELECT id, text FROM verses
            ''')
            print("âœ… ØªÙ… Ù…Ù„Ø¡ ÙÙ‡Ø±Ø³ FTS5 Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        else:
            print(f"âœ… ÙÙ‡Ø±Ø³ FTS5 Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„: {count} Ø¢ÙŠØ©")
        
        conn.commit()
        conn.close()
        
        elapsed = time.time() - start_time
        print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5 ÙÙŠ {elapsed:.1f} Ø«Ø§Ù†ÙŠØ©")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5: {e}")
        return False

def fix_fts_arabic_search():
    """
    ğŸ”§ Ø¥ØµÙ„Ø§Ø­ FTS5 Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
    """
    try:
        conn = sqlite3.connect('quran.db')
        cursor = conn.cursor()
        
        # 1. Ø¥Ø³Ù‚Ø§Ø· Ø¬Ø¯ÙˆÙ„ FTS5 Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        cursor.execute("DROP TABLE IF EXISTS verses_fts")
        
        # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ FTS5 Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ tokenizer Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        cursor.execute('''
            CREATE VIRTUAL TABLE verses_fts 
            USING fts5(
                text,
                content='verses',
                content_rowid='id',
                tokenize='porter unicode61'  # âœ… ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            )
        ''')
        
        # 3. Ù…Ù„Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
        cursor.execute('''
            INSERT INTO verses_fts(rowid, text)
            SELECT id, text FROM verses
        ''')
        
        # 4. Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_fts_text ON verses_fts(text)')
        
        conn.commit()
        conn.close()
        
        print("âœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­ FTS5 Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ØµÙ„Ø§Ø­ FTS5: {e}")
        return False

def rebuild_fts_for_arabic(db: Session):
    """
    Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ FTS5 Ù…Ø¹ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    """
    print("ğŸ”§ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ FTS5 Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...")
    success = fix_fts_arabic_search()
    
    if success:
        global FTS_AVAILABLE
        FTS_AVAILABLE = True
        print("ğŸ‰ FTS5 Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ")
    
    return success

# ============================================
# ğŸš€ Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ Ù…Ø³Ø±Ù‘Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache
# ============================================

def fast_all_similarities_from_cache(db: Session, target_verses: List[Verse], compare_verses: List[Verse], 
                                   min_similarity: float, limit: int, exclude_basmala: bool):
    """
    ğŸš€ Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ Ù…Ø³Ø±Ù‘Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 1-10 Ø«ÙˆØ§Ù†Ù Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 300+ Ø«Ø§Ù†ÙŠØ©
    """
    print("ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹...")
    start_time = time.time()
    
    similarities = []
    seen_pairs = set()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹
    target_verse_ids = {v.id for v in target_verses}
    compare_verse_ids = {v.id for v in compare_verses}
    
    processed = 0
    total_target = len(target_verses)
    
    for target_verse in target_verses:
        processed += 1
        if processed % 50 == 0:
            elapsed_so_far = time.time() - start_time
            print(f"   ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {processed}/{total_target} Ø¢ÙŠØ© ({elapsed_so_far:.1f}Ø«ØŒ {len(similarities)} Ù…ØªØ´Ø§Ø¨Ù‡)")
        
        # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨Ø³Ù…Ù„Ø© ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§ Ù…Ø·Ù„ÙˆØ¨
        if exclude_basmala and is_basmala_verse(target_verse):
            continue
            
        # Ø¬Ù„Ø¨ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©
        if target_verse.id in SIMILARITY_CACHE:
            cached_similarities = SIMILARITY_CACHE[target_verse.id]
            
            for sim in cached_similarities:
                compare_id = sim['verse_id']
                
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¢ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                if compare_id not in compare_verse_ids:
                    continue
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ù†ÙØ³ Ø§Ù„Ø¢ÙŠØ©
                if compare_id == target_verse.id:
                    continue
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                pair = tuple(sorted([target_verse.id, compare_id]))
                if pair in seen_pairs:
                    continue
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
                if sim['similarity'] >= min_similarity:
                    # Ø¬Ù„Ø¨ Ø§Ù„Ø¢ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    compare_verse = db.query(Verse).filter(Verse.id == compare_id).first()
                    if not compare_verse:
                        continue
                    
                    # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨Ø³Ù…Ù„Ø© ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§ Ù…Ø·Ù„ÙˆØ¨
                    if exclude_basmala and is_basmala_verse(compare_verse):
                        continue
                    
                    # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ù„Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª 100%
                    if not is_excluded_100_percent_match(target_verse.text, compare_verse.text):
                        seen_pairs.add(pair)
                        similarities.append({
                            'verse1': target_verse.to_dict(),
                            'verse2': compare_verse.to_dict(),
                            'similarity': sim['similarity'],
                            'score_percent': int(sim['similarity'] * 100)
                        })
                    
                    if len(similarities) >= limit:
                        break
        
        if len(similarities) >= limit:
            break
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    similarities.sort(key=lambda x: x['similarity'], reverse=True)
    
    elapsed = time.time() - start_time
    print(f"âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹: {len(similarities)} Ù†ØªÙŠØ¬Ø© ÙÙŠ {elapsed:.1f}Ø«")
    
    return similarities

# ============================================
# ğŸ† Ø¯ÙˆØ§Ù„ Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±
# ============================================

def load_mutashabihat_bank():
    """ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ùƒ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† JSON"""
    global MUTASHABIHAT_BANK
    
    json_path = FilePath(__file__).parent / "mutashabihat_kalima.json"
    
    if not json_path.exists():
        print("âš ï¸ Ù…Ù„Ù mutashabihat_kalima.json ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return None
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            MUTASHABIHAT_BANK = json.load(f)
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª: {MUTASHABIHAT_BANK['total_questions']} Ø³Ø¤Ø§Ù„")
        return MUTASHABIHAT_BANK
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ù†Ùƒ: {e}")
        return None

def get_expert_distinguish_question(db: Session, scope_filter):
    """
    ğŸ† ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ø¦Ù„Ø© Ù…Ù† "Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙƒÙ„Ù…Ø©"
    
    âœ… ØªØ­Ø³ÙŠÙ†Ø§Øª:
    - Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ø®ÙŠØ§Ø± ØµØ­ÙŠØ­ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
    - Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ© ÙÙ‚Ø·
    """
    global MUTASHABIHAT_BANK
    
    if not MUTASHABIHAT_BANK:
        print("âš ï¸ Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ØºÙŠØ± Ù…Ø­Ù…Ù„ØŒ Ù†Ø±Ø¬Ø¹ Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©")
        return get_distinguish_question(db, scope_filter, 0.85)
    
    questions = MUTASHABIHAT_BANK['questions']
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø³Ø¤Ø§Ù„ Ù…Ù†Ø§Ø³Ø¨ (10 Ù…Ø­Ø§ÙˆÙ„Ø§Øª)
    for attempt in range(10):
        # Ø§Ø®ØªÙŠØ§Ø± Ø³Ø¤Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
        question_data = random.choice(questions)
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¢ÙŠØ§Øª ÙƒØ§ÙÙŠØ© (Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 2)
        if len(question_data['verses']) < 2:
            continue
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯Ù‡Ø§ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚
        valid_verses = []
        
        for verse_info in question_data['verses']:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¢ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            verse = db.query(Verse).filter(
                Verse.surah == verse_info['surah'],
                Verse.ayah == verse_info['ayah']
            ).first()
            
            if verse:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø¢ÙŠØ© ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯
                if scope_filter == True or db.query(Verse).filter(
                    Verse.id == verse.id,
                    scope_filter
                ).first():
                    valid_verses.append(verse)
        
        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ 2+ Ø¢ÙŠØ§Øª ØµØ§Ù„Ø­Ø© ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚
        if len(valid_verses) >= 2:
            # ğŸ¯ Ø§Ø®ØªÙŠØ§Ø± Ø¢ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙƒØ¥Ø¬Ø§Ø¨Ø© ØµØ­ÙŠØ­Ø©
            correct_verse = random.choice(valid_verses)
            
            # ğŸ¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ ÙƒØ®ÙŠØ§Ø±Ø§Øª (Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ© ÙÙ‚Ø·)
            other_verses = [
                v for v in valid_verses 
                if v.id != correct_verse.id and v.surah != correct_verse.surah  # âœ… Ø³ÙˆØ±Ø© Ù…Ø®ØªÙ„ÙØ© ÙÙ‚Ø·
            ]
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¢ÙŠØ§Øª Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ©ØŒ Ù†Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø¨Ø­Ø«
            if len(other_verses) < 1:
                continue
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª (2-3 Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ©)
            options = [correct_verse.text]
            for v in other_verses[:3]:
                options.append(v.text)
            
            # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬: Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
            options = list(set(options))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
            if options.count(correct_verse.text) > 1:
                continue  # Ù†Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³Ø¤Ø§Ù„ Ø¢Ø®Ø±
            
            random.shuffle(options)
            
            print(f"âœ… expert_mode: Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ø¨Ù†Ùƒ")
            print(f"   Category: {question_data['category']}")
            print(f"   Pattern: {question_data['pattern']}")
            print(f"   Correct: {correct_verse.surah_name} ({correct_verse.surah}:{correct_verse.ayah})")
            print(f"   Options: {len(options)} (Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ©)")
            
            # âœ… Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯
            for i, opt in enumerate(options, 1):
                is_correct = "âœ“" if opt == correct_verse.text else ""
                print(f"      {i}. {opt[:50]}... {is_correct}")
            
            return {
                "question_type": "distinguish",
                "question_text": f"Ø£ÙŠ Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø³ÙˆØ±Ø© **{correct_verse.surah_name}**ØŸ\n\n(ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ†)",
                "correct_answer": correct_verse.text,
                "verse_info": {
                    "surah_name": correct_verse.surah_name,
                    "surah": correct_verse.surah,
                    "ayah": correct_verse.ayah
                },
                "options": options,
                "expert_mode": True,
                "bank_question_id": question_data['id'],
                "category": question_data['category']
            }
    
    # Ø¥Ø°Ø§ ÙØ´Ù„Ù†Ø§ ÙÙŠ Ø¥ÙŠØ¬Ø§Ø¯ Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ø¨Ù†ÙƒØŒ Ù†Ø±Ø¬Ø¹ Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
    print("âš ï¸ Ù„Ù… Ù†Ø¬Ø¯ Ø³Ø¤Ø§Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ù† Ø§Ù„Ø¨Ù†ÙƒØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©")
    return get_distinguish_question(db, scope_filter, 0.85)

# ============================================
# ğŸ” Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙˆØ§Ù„Ù…ÙØ³Ø±Ù‘Ø¹Ø©
# ============================================

def exact_text_search(db: Session, query: str, limit: int = 20) -> List[dict]:
    """
    ğŸ”¥ Ø¨Ø­Ø« Ù†ØµÙŠ Ø¯Ù‚ÙŠÙ‚ ÙˆØ³Ø±ÙŠØ¹ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 5-50ms Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 500ms
    âœ… ÙŠØ¯Ø¹Ù… Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ÙˆØ§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
    """
    start_time = time.time()
    
    original_query = query.strip()
    query_clean = clean_text(query)
    
    print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ù†: '{original_query}' (Ù†Ø¸ÙŠÙ: '{query_clean}')")
    
    exact_matches = []
    seen_ids = set()
    
    # ============================================
    # âœ… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)
    # ============================================
    if original_query:
        # ğŸ”¥ SQL Ù…Ø¨Ø§Ø´Ø± - Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹
        original_verses = db.query(Verse).filter(
            Verse.text.contains(original_query)
        ).limit(limit).all()
        
        for verse in original_verses:
            if verse.id not in seen_ids:
                exact_matches.append({
                    **verse.to_dict(),
                    'similarity': "1.0000",
                    'match_type': 'exact_original'
                })
                seen_ids.add(verse.id)
    
    # ============================================
    # âœ… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ (Ø§Ù„Ø¹Ø§Ø¯ÙŠ)
    # ============================================
    if len(exact_matches) < limit and query_clean:
        # ğŸ”¥ Ù†Ø­ØªØ§Ø¬ Ù„Ø·Ø±ÙŠÙ‚Ø© Ø£Ø°ÙƒÙ‰ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù†Ø¸ÙŠÙ
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª (1000 Ø¢ÙŠØ© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
        all_verses = db.query(Verse).limit(1000).all()
        
        for verse in all_verses:
            if len(exact_matches) >= limit:
                break
                
            if verse.id in seen_ids:
                continue
            
            verse_clean = clean_text(verse.text)
            
            if query_clean in verse_clean:
                exact_matches.append({
                    **verse.to_dict(),
                    'similarity': "1.0000",
                    'match_type': 'exact_clean'
                })
                seen_ids.add(verse.id)
    
    elapsed = time.time() - start_time
    print(f"âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚: {len(exact_matches)} Ù†ØªÙŠØ¬Ø© ÙÙŠ {elapsed:.3f}Ø«")
    
    return exact_matches

def fallback_search(db: Session, query: str, limit: int = 20, threshold: float = 0.7, error: str = None):
    """
    ğŸ”¥ Ø¨Ø­Ø« Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù…Ø­Ø³Ù‘Ù† ÙˆØ³Ø±ÙŠØ¹
    âš¡ Ù„Ø§ ÙŠØ¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø¢ÙŠØ§ØªØŒ Ø¨Ù„ ÙŠØ¨Ø­Ø« Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ
    """
    start_time = time.time()
    query_clean = clean_text(query)
    
    print(f"ğŸ” Ø¨Ø­Ø« Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù…Ø­Ø³Ù‘Ù†: '{query}' (Ù†Ø¸ÙŠÙ: '{query_clean}')")
    
    # ============================================
    # ğŸ”¥ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø°ÙƒÙŠØ©: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¢ÙŠØ§Øª Ù…Ø®ØªØ§Ø±Ø©
    # ============================================
    
    # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (500 Ø¢ÙŠØ© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
    sample_verses = db.query(Verse).order_by(func.random()).limit(500).all()
    
    # 2. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¢ÙŠØ§Øª Ø¨Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØ±ÙƒØ©
    query_words = query_clean.split()
    if len(query_words) > 0:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢ÙŠØ§Øª Ø¨Ù‡Ø§ Ø£ÙŠ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        additional_verses = []
        for word in query_words[:3]:  # Ø£ÙˆÙ„ 3 ÙƒÙ„Ù…Ø§Øª ÙÙ‚Ø·
            if len(word) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
                word_verses = db.query(Verse).filter(
                    Verse.text.contains(word)
                ).limit(100).all()
                additional_verses.extend(word_verses)
        
        sample_verses.extend(additional_verses)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    verse_ids = set()
    unique_verses = []
    for verse in sample_verses:
        if verse.id not in verse_ids:
            verse_ids.add(verse.id)
            unique_verses.append(verse)
    
    print(f"   ğŸ“Š Ø¹ÙŠÙ†Ø© Ø§Ù„Ø¨Ø­Ø«: {len(unique_verses)} Ø¢ÙŠØ©")
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    final_results = []
    
    for verse in unique_verses:
        verse_clean = clean_text(verse.text)
        similarity = calculate_similarity(query_clean, verse_clean)
        
        if similarity >= threshold:
            verse_dict = verse.to_dict()
            verse_dict['similarity'] = f"{similarity:.4f}"
            verse_dict['match_type'] = 'lexical'
            final_results.append((verse_dict, similarity))
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    final_results.sort(key=lambda x: x[1], reverse=True)
    final_results = final_results[:limit]
    
    elapsed = time.time() - start_time
    
    return {
        "query": query,
        "search_time": f"{elapsed:.3f}s",
        "error": error if error else "ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ",
        "total_found": len(final_results),
        "results": [item[0] for item in final_results]
    }

def semantic_search(query: str, limit: int = 100):
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS - Ù…Ø¹Ø·Ù„ ÙÙŠ Production"""
    print(f"âš ï¸ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù…Ø¹Ø·Ù„ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: '{query}'")
    print("ğŸ’¡ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†Ù‡ (Ø£Ø³Ø±Ø¹ ÙˆØ£Ø¯Ù‚)")
    return []  # Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©

# ============================================
# ğŸ†• Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ
# ============================================

def optimized_text_search(db: Session, query: str, limit: int = 20) -> List[dict]:
    """
    ğŸ” Ø¨Ø­Ø« Ù†ØµÙŠ Ù…Ø­Ø³Ù‘Ù† ÙˆØ³Ø±ÙŠØ¹
    âœ… ÙŠØ¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø©
    âš¡ Ø£Ø³Ø±Ø¹ 10-100 Ù…Ø±Ø© Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    """
    start_time = time.time()
    
    original_query = query.strip()
    query_clean = clean_text(query)
    
    print(f"ğŸ” Ø¨Ø­Ø« Ù†ØµÙŠ Ù…Ø­Ø³Ù‘Ù†: '{original_query}'")
    
    results = []
    seen_ids = set()
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙÙŠ SQL (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)
    # ============================================
    if original_query:
        direct_matches = db.query(Verse).filter(
            Verse.text.contains(original_query)
        ).limit(limit).all()
        
        for verse in direct_matches:
            results.append({
                **verse.to_dict(),
                'similarity': '1.0000',
                'match_type': 'direct_sql'
            })
            seen_ids.add(verse.id)
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FTS5 Ø¥Ø°Ø§ Ù…ØªØ§Ø­
    # ============================================
    if len(results) < limit and FTS_AVAILABLE:
        fts_results = fast_text_search_fts(query, limit - len(results))
        
        for result in fts_results:
            if result['id'] not in seen_ids:
                results.append(result)
                seen_ids.add(result['id'])
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†Ø¸ÙŠÙ Ø§Ù„Ø°ÙƒÙŠ
    # ============================================
    if len(results) < limit and query_clean:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        query_words = query_clean.split()
        
        if len(query_words) == 1:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø¨Ø­Ø« Ø¨Ø³ÙŠØ·
            sample_verses = db.query(Verse).limit(500).all()
            
            for verse in sample_verses:
                if len(results) >= limit:
                    break
                    
                if verse.id in seen_ids:
                    continue
                
                verse_clean = clean_text(verse.text)
                
                if query_clean in verse_clean:
                    results.append({
                        **verse.to_dict(),
                        'similarity': '1.0000',
                        'match_type': 'clean_match'
                    })
                    seen_ids.add(verse.id)
        
        elif len(query_words) > 1:
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¹Ø¨Ø§Ø±Ø©ØŒ Ø¨Ø­Ø« Ø°ÙƒÙŠ
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢ÙŠØ§Øª Ø¨Ù‡Ø§ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
            first_word = query_words[0]
            if len(first_word) > 2:
                candidate_verses = db.query(Verse).filter(
                    Verse.text.contains(first_word)
                ).limit(200).all()
                
                for verse in candidate_verses:
                    if len(results) >= limit:
                        break
                        
                    if verse.id in seen_ids:
                        continue
                    
                    verse_clean = clean_text(verse.text)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
                    if all(word in verse_clean for word in query_words):
                        results.append({
                            **verse.to_dict(),
                            'similarity': '1.0000',
                            'match_type': 'phrase_match'
                        })
                        seen_ids.add(verse.id)
    
    elapsed = time.time() - start_time
    print(f"âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: {len(results)} Ù†ØªÙŠØ¬Ø© ÙÙŠ {elapsed:.3f}Ø«")
    
    return results

# ============================================
# ğŸ†• Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
# ============================================
def get_word_distractors(db: Session, target_word: str, current_surah_id: int, limit: int = 3) -> List[str]:
    """Ø¬Ù„Ø¨ ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØªØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
    # ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØªØ© Ø´Ø§Ø¦Ø¹Ø©
    distractors = ["Ø§Ù„Ø³Ù…Ø§Ø¡", "Ø§Ù„Ø£Ø±Ø¶", "Ø§Ù„Ù†Ø§Ø³", "Ø§Ù„Ø°ÙŠ", "ÙˆÙ‡Ù…", "Ø§Ù„Ø°ÙŠÙ†", "Ø§Ù„Ù„Ù‡", "Ø§Ù„Ø±Ø­Ù…Ù†", "Ø§Ù„Ø±Ø­ÙŠÙ…"]
    
    try:
        # Ø¬Ù„Ø¨ Ø¢ÙŠØ§Øª Ù‚Ø±ÙŠØ¨Ø© (Ù†Ø·Ø§Ù‚ Ø³ÙˆØ±Ø©)
        nearby_verses = db.query(Verse).filter(Verse.surah == current_surah_id).limit(10).all()
        
        all_words = []
        for v in nearby_verses:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø¹Ø¯ ØªÙ†Ø¸ÙŠÙÙ‡Ø§
            words = [clean_text(w) for w in v.text.split() if len(w) > 2 and clean_text(w) != clean_text(target_word)]
            all_words.extend(words)
        
        # Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙØ±ÙŠØ¯Ø©
        unique_words = list(set(all_words))
        
        # ØªØµÙÙŠØ© ÙˆØ¥Ø¶Ø§ÙØ© Ù…Ø´ØªØªØ§Øª ÙØ±ÙŠØ¯Ø©
        selected_distractors = [d for d in unique_words if d != clean_text(target_word)]
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ 3 Ù…Ø´ØªØªØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
        if len(selected_distractors) < 3:
            selected_distractors.extend(distractors)
            selected_distractors = list(set(selected_distractors))
            
        return random.sample(selected_distractors, min(limit, len(selected_distractors)))
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø´ØªØªØ§Øª: {e}")
        return random.sample(distractors, min(limit, len(distractors)))

def get_verse_distractor(db: Session, target_verse: Verse, threshold: float, limit: int = 5) -> Optional[Verse]:
    """Ø¬Ù„Ø¨ Ø¢ÙŠØ© Ù…Ø´ØªØªØ© Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢ÙŠØ© Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù…Ù† Ø³ÙˆØ±Ø© Ø£Ø®Ø±Ù‰
    try:
        # Ø¬Ù„Ø¨ Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø³ÙˆØ±Ø© Ø£Ø®Ø±Ù‰
        other_verses = db.query(Verse).filter(Verse.surah != target_verse.surah, Verse.juz != target_verse.juz).limit(100).all()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢ÙŠØ© ØªØ´Ø§Ø¨Ù‡Ù‡Ø§ Ù„ÙØ¸ÙŠØ§Ù‹ Ø¨Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ©
        similar_distractors = []
        target_clean = clean_text(target_verse.text)
        
        for v in other_verses:
            v_clean = clean_text(v.text)
            similarity = calculate_word_similarity(target_clean, v_clean)
            
            # Ù†Ø®ØªØ§Ø± Ø¢ÙŠØ© Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¬Ø¯Ø§Ù‹ØŒ ÙˆÙ„ÙƒÙ† Ù„ÙŠØ³Øª 100%
            if 0.75 < similarity < 0.95:
                similar_distractors.append((v, similarity))
                
        if similar_distractors:
            # Ù†Ø®ØªØ§Ø± Ø§Ù„Ø¢ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹
            similar_distractors.sort(key=lambda x: x[1], reverse=True)
            return similar_distractors[0][0]
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø±Ø¬Ø¹ Ø¢ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù‚ØµÙŠØ±Ø©
        return db.query(Verse).filter(Verse.surah != target_verse.surah).order_by(func.random()).first()
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø´ØªØª Ø§Ù„Ø¢ÙŠØ©: {e}")
        return db.query(Verse).filter(Verse.surah != target_verse.surah).order_by(func.random()).first()

def get_word_choice_question(db: Session, scope_filter, threshold: float):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¤Ø§Ù„ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª"""
    random_verse = db.query(Verse).filter(scope_filter).order_by(func.random()).first()
    
    if not random_verse:
        return {"error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¢ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±."}
    
    # 1. Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø­Ø°ÙÙ‡Ø§
    words = random_verse.text.split()
    if len(words) < 5:
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¢ÙŠØ© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø¬Ù„Ø¨ Ø¢ÙŠØ© Ø£Ø®Ø±Ù‰.
        return get_word_choice_question(db, scope_filter, threshold) 
        
    # Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„ÙŠØ³Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø£Ùˆ Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    word_index = random.randint(1, len(words) - 2)
    correct_word = words[word_index]
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„
    question_text = " ".join(words[:word_index]) + " (___) " + " ".join(words[word_index+1:])
    
    # 2. Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø´ØªØªØ§Øª
    distractors = get_word_distractors(db, correct_word, random_verse.surah, limit=3)
    
    # 3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
    options = [correct_word] + distractors
    random.shuffle(options)
    
    return {
        "question_type": "word_choice",
        "question_text": question_text,
        "correct_answer": correct_word,
        "verse_info": {
            "surah_name": random_verse.surah_name,
            "surah": random_verse.surah,
            "ayah": random_verse.ayah
        },
        "options": options # ğŸ’¡ Ø­Ù‚Ù„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯
    }

def get_distinguish_question(db: Session, scope_filter, threshold: float):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¤Ø§Ù„ ØªÙ…ÙŠÙŠØ² Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª"""
    random_verse = db.query(Verse).filter(scope_filter).order_by(func.random()).first()
    
    if not random_verse:
        return {"error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¢ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±."}
    
    # 1. Ø¬Ù„Ø¨ Ø¢ÙŠØ© Ù…Ø´ØªØªØ© Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¬Ø¯Ø§Ù‹ ÙˆÙ„ÙƒÙ† Ù…Ù† Ø³ÙˆØ±Ø© Ø£Ø®Ø±Ù‰
    distractor_verse = get_verse_distractor(db, random_verse, threshold)
    
    if not distractor_verse:
        # Ø¥Ø°Ø§ ÙØ´Ù„Ù†Ø§ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø´ØªØª Ø¬ÙŠØ¯ØŒ Ù†Ø¹ÙˆØ¯ Ù„Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© (Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù‚Ø¯ÙŠÙ…)
        return get_word_choice_question(db, scope_filter, threshold) 
    
    # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø³Ø¤Ø§Ù„
    question_text = f"Ø£ÙŠ Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø³ÙˆØ±Ø© **{random_verse.surah_name}**ØŸ"
    
    # 3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
    correct_option = random_verse.text
    distractor_option = distractor_verse.text
    
    options = [correct_option, distractor_option]
    random.shuffle(options)
    
    return {
        "question_type": "distinguish",
        "question_text": question_text,
        "correct_answer": correct_option, # Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ Ù†Øµ Ø§Ù„Ø¢ÙŠØ©
        "verse_info": {
            "surah_name": random_verse.surah_name,
            "surah": random_verse.surah,
            "ayah": random_verse.ayah
        },
        "options": options # ğŸ’¡ Ø­Ù‚Ù„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯
    }

# ============================================
# Ø¥Ø¹Ø¯Ø§Ø¯ FastAPI
# ============================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ø¯Ø§Ù„Ø© ØªÙÙ†ÙØ° Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„"""
    print("\n--- ğŸ’¾ Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite ---")
    db = next(get_db())
    init_db(db)
    
    # ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    initialize_search_engine(db)
    
    # ğŸ† ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª
    load_mutashabihat_bank()
    
    # ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
    initialize_optimizations(db)
    
    db.close()
    yield
    print("\n--- ğŸ§¹ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø®Ø§Ø¯Ù… ---")

app = FastAPI(
    title="Ø§Ù„Ù…ØµØ­Ù Ø§Ù„Ø°ÙƒÙŠ API",
    description="API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© (Ù‡Ø¬ÙŠÙ† Ø°ÙƒÙŠ: Ø³Ø±ÙŠØ¹ + Ù„ÙØ¸ÙŠ) + ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ† + ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø³Ø±Ø¹Ø© ğŸš€",
    version="5.3.0",  # âœ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥ØµØ¯Ø§Ø±
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,  # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØºÙŠØ±
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# ğŸ†• ENDPOINT Ø§Ù„Ø¬Ø¯ÙŠØ¯: Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================

@app.get("/")
def root():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„API"""
    return {
        "message": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…ØµØ­Ù AI API ğŸ•Œ",
        "version": "5.3.0",
        "status": "ÙŠØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­ ğŸš€",
        "endpoints": {
            "search": "/search?q=Ø§Ù„ÙƒÙ„Ù…Ø©",
            "search_fixed": "/search/fixed?q=Ø§Ù„ÙƒÙ„Ù…Ø© (ÙŠØ¯Ø¹Ù… Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)",
            "live_search": "/search/live?q=Ø§Ù„ÙƒÙ„Ù…Ø©",
            "similar_verses": "/similar/{verse_id}",
            "quiz": "/quiz/get_question (POST)",
            "stats": "/stats",
            "performance": "/performance/stats",
            "documentation": "/docs"
        },
        "note": "Ø²ÙˆØ± /docs Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"
    }

# ============================================
# ğŸ†• ENDPOINT Ø§Ù„Ø¬Ø¯ÙŠØ¯: Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
# ============================================

@app.get("/search/fixed")
def fixed_search(
    q: str = Query(..., min_length=1),
    limit: int = Query(20, gt=0, le=100),
    highlight: bool = Query(True, description="ØªØ¸Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    db: Session = Depends(get_db)
):
    """
    ğŸ” Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù† ÙŠØ¯Ø¹Ù… Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ÙˆØ§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
    âœ… ÙŠØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)
    âœ… ÙŠØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ (Ø§Ù„Ø¹Ø§Ø¯ÙŠ)
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 5-50ms
    """
    print(f"\nğŸ¯ Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ: '{q}'")
    start_time = time.time()
    
    results = []
    seen_ids = set()
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)
    # ============================================
    if q:
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙÙŠ SQL
        verses = db.query(Verse).filter(
            Verse.text.contains(q)
        ).limit(limit).all()
        
        for verse in verses:
            verse_dict = verse.to_dict()
            verse_dict['similarity'] = '1.0000'
            verse_dict['match_type'] = 'exact_original'
            
            if highlight:
                verse_dict['highlighted_text'] = highlight_words_in_text(verse.text, q)
            
            results.append(verse_dict)
            seen_ids.add(verse.id)
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ (Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ø§Ù„Ù…Ø²ÙŠØ¯)
    # ============================================
    if len(results) < limit:
        q_clean = clean_text(q)
        
        if q_clean and q_clean != clean_text(q):  # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ ØºÙŠØ± Ø§Ù„ØªØ§Ù…
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ
            all_verses = db.query(Verse).limit(1000).all()  # ğŸ”¥ Ø¹ÙŠÙ†Ø© Ø°ÙƒÙŠØ©
            
            for verse in all_verses:
                if len(results) >= limit:
                    break
                    
                if verse.id in seen_ids:
                    continue
                
                verse_clean = clean_text(verse.text)
                
                if q_clean in verse_clean:
                    verse_dict = verse.to_dict()
                    verse_dict['similarity'] = '1.0000'
                    verse_dict['match_type'] = 'exact_clean'
                    
                    if highlight:
                        verse_dict['highlighted_text'] = highlight_words_in_text(verse.text, q)
                    
                    results.append(verse_dict)
                    seen_ids.add(verse.id)
    
    # ============================================
    # ğŸ”¥ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… FTS5 ÙƒØ¨Ø¯ÙŠÙ„ (Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬)
    # ============================================
    if len(results) == 0 and FTS_AVAILABLE:
        print("   âš¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± FTS5...")
        fts_results = fast_text_search_fts(q, limit)
        
        for result in fts_results:
            if result['id'] not in seen_ids:
                if highlight:
                    result['highlighted_text'] = highlight_words_in_text(result['text'], q)
                results.append(result)
                seen_ids.add(result['id'])
    
    elapsed = time.time() - start_time
    
    return {
        "query": q,
        "query_clean": clean_text(q) if len(results) < limit else None,
        "search_time": f"{elapsed:.3f}s",
        "total_found": len(results),
        "match_type": "exact_original" if len(results) > 0 else "no_match",
        "method": "contains_search",
        "results": results
    }

@app.get("/search/both")
def search_both_methods(
    q: str = Query(..., min_length=1),
    limit: int = Query(20, gt=0, le=100),
    highlight: bool = Query(True, description="ØªØ¸Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    db: Session = Depends(get_db)
):
    """
    ğŸ”¥ Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ ÙŠØ¯Ø¹Ù… Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ© Ù…Ø¹Ø§Ù‹
    âœ… ÙŠØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ) ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ (Ø§Ù„Ø¹Ø§Ø¯ÙŠ)
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 20-100ms
    """
    print(f"\nğŸ¯ Ø¨Ø­Ø« Ø´Ø§Ù…Ù„: '{q}'")
    start_time = time.time()
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø·Ø±ÙŠÙ‚ØªÙŠÙ† Ù…Ø¹Ø§Ù‹
    results = []
    seen_ids = set()
    
    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ)
    verses_original = db.query(Verse).filter(
        Verse.text.contains(q)
    ).limit(limit).all()
    
    for verse in verses_original:
        verse_dict = verse.to_dict()
        verse_dict['similarity'] = '1.0000'
        verse_dict['match_type'] = 'exact_original'
        verse_dict['method'] = 'contains_original'
        
        if highlight:
            verse_dict['highlighted_text'] = highlight_words_in_text(verse.text, q)
        
        results.append(verse_dict)
        seen_ids.add(verse.id)
    
    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ (Ø§Ù„Ø¹Ø§Ø¯ÙŠ) - ÙÙ‚Ø· Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ø§Ù„Ù…Ø²ÙŠØ¯
    if len(results) < limit:
        q_clean = clean_text(q)
        
        # ğŸ”¥ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ: Ù†Ø¨Ø­Ø« ÙÙŠ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª ÙÙ‚Ø·
        sample_verses = db.query(Verse).limit(500).all()
        
        for verse in sample_verses:
            if len(results) >= limit:
                break
                
            if verse.id in seen_ids:
                continue
            
            verse_clean = clean_text(verse.text)
            if q_clean in verse_clean:
                verse_dict = verse.to_dict()
                verse_dict['similarity'] = '1.0000' 
                verse_dict['match_type'] = 'exact_clean'
                verse_dict['method'] = 'contains_clean'
                
                if highlight:
                    verse_dict['highlighted_text'] = highlight_words_in_text(verse.text, q)
                
                results.append(verse_dict)
                seen_ids.add(verse.id)
    
    elapsed = time.time() - start_time
    
    return {
        "query": q,
        "query_clean": clean_text(q),
        "search_time": f"{elapsed:.3f}s",
        "total_found": len(results),
        "match_type": "both_methods",
        "results": results[:limit]
    }

# ============================================
# ğŸš€ endpoints Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
# ============================================

@app.get("/search/live")
def live_search_verses(
    q: str = Query(..., min_length=1, description="Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«"),
    limit: int = Query(20, gt=0, le=50, description="Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    highlight: bool = Query(True, description="ØªØ¸Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    db: Session = Depends(get_db)
):
    """
    ğŸš€ Ø¨Ø­Ø« ÙÙˆØ±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FTS5
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 5-20ms
    âœ… Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø©
    """
    print(f"\nğŸ¯ Ø¨Ø­Ø« ÙÙˆØ±ÙŠ: '{q}'")
    start_time = time.time()
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FTS5
    results = fast_text_search_fts(q, limit)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¸Ù„ÙŠÙ„ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
    if highlight:
        for result in results:
            result['highlighted_text'] = highlight_words_in_text(result['text'], q)
    
    elapsed = time.time() - start_time
    
    return {
        "query": q,
        "search_time": f"{elapsed:.3f}s",
        "total_found": len(results),
        "match_type": "fts_live",
        "method": "FTS5",
        "results": results
    }

@app.get("/similarities/fast/{verse_id}")
def get_fast_similarities(
    verse_id: int = Path(..., description="Ù…Ø¹Ø±Ù Ø§Ù„Ø¢ÙŠØ©"),
    min_similarity: float = Query(0.6, ge=0.3, le=1.0, description="Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡"),
    db: Session = Depends(get_db)
):
    """
    ğŸš€ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙÙˆØ±ÙŠØ© Ù…Ù† Cache
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 10-50ms (Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø®Ø²Ù†Ø©)
    âœ… ÙŠØ¹Ù…Ù„ Ø­ØªÙ‰ Ø¨Ø¯ÙˆÙ† Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
    """
    print(f"\nğŸ¯ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙÙˆØ±ÙŠØ© Ù„Ù„Ø¢ÙŠØ©: {verse_id}")
    start_time = time.time()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¢ÙŠØ©
    verse = db.query(Verse).filter(Verse.id == verse_id).first()
    if not verse:
        raise HTTPException(status_code=404, detail="Ø§Ù„Ø¢ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
    
    # Ø¬Ù„Ø¨ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù…Ù† cache
    similar_verses = get_cached_similarities(verse_id, min_similarity)
    
    elapsed = time.time() - start_time
    
    return {
        "verse": verse.to_dict(),
        "similar_verses": similar_verses,
        "search_time": f"{elapsed:.3f}s",
        "method": "cache",
        "cache_hit": len(similar_verses) > 0,
        "total_found": len(similar_verses)
    }

@app.get("/autocomplete/{prefix}")
def get_autocomplete_suggestions(
    prefix: str = Path(..., min_length=2, description="Ø¨Ø§Ø¯Ø¦Ø© Ø§Ù„ÙƒÙ„Ù…Ø©"),
    limit: int = Query(10, gt=0, le=20, description="Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª")
):
    """
    ğŸš€ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø©
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 1-5ms
    âœ… ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ word statistics cache
    """
    print(f"\nğŸ¯ AutoComplete: '{prefix}'")
    start_time = time.time()
    
    prefix_clean = clean_text(prefix)
    suggestions = []
    
    if WORD_STATS_CACHE:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©
        for word, stats in WORD_STATS_CACHE.items():
            if word.startswith(prefix_clean):
                suggestions.append({
                    'word': word,
                    'count': stats['total_count'],
                    'verses_count': stats['verses_count']
                })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        suggestions.sort(key=lambda x: x['count'], reverse=True)
        suggestions = suggestions[:limit]
    
    elapsed = time.time() - start_time
    
    return {
        "prefix": prefix,
        "suggestions": suggestions,
        "search_time": f"{elapsed:.3f}s",
        "total_found": len(suggestions)
    }

@app.get("/admin/build-fts")
def admin_build_fts_index(db: Session = Depends(get_db)):
    """
    ğŸ”§ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5 (Ù„Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ†)
    âš ï¸ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù - ÙŠÙØ´ØºÙ‘Ù„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
    """
    print("\nğŸ”§ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5...")
    success = build_fts_index(db)
    
    return {
        "success": success,
        "message": "ØªÙ… Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5 Ø¨Ù†Ø¬Ø§Ø­" if success else "ÙØ´Ù„ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FTS5"
    }

# âœ… Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©:
@app.get("/admin/fix-fts")
def admin_fix_fts_index(db: Session = Depends(get_db)):
    """
    ğŸ”§ Ø¥ØµÙ„Ø§Ø­ ÙÙ‡Ø±Ø³ FTS5 Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ
    âš ï¸ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù
    """
    print("\nğŸ”§ Ø¥ØµÙ„Ø§Ø­ FTS5 Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ...")
    success = rebuild_fts_for_arabic(db)
    
    return {
        "success": success,
        "message": "ØªÙ… Ø¥ØµÙ„Ø§Ø­ FTS5 Ø¨Ù†Ø¬Ø§Ø­" if success else "ÙØ´Ù„ Ø¥ØµÙ„Ø§Ø­ FTS5"
    }

@app.get("/admin/build-cache")
def admin_build_cache(
    cache_type: str = Query("all", pattern="^(all|similarity|word_stats)$"),
    min_similarity: float = Query(0.1, ge=0.05, le=1.0),  # âœ… Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù…Ù„ Ø¬Ø¯ÙŠØ¯
    db: Session = Depends(get_db)
):
    """
    ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Cache (Ù„Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ†)
    """
    print(f"\nğŸ”§ Ø¨Ù†Ø§Ø¡ {cache_type} cache...")
    
    results = {}
    
    if cache_type in ["all", "similarity"]:
        # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… min_similarity Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        results['similarity_cache'] = build_similarity_cache(db, min_similarity)
    
    if cache_type in ["all", "word_stats"]:
        results['word_stats_cache'] = build_word_statistics_cache(db)
    
    return {
        "success": True,
        "message": f"ØªÙ… Ø¨Ù†Ø§Ø¡ {cache_type} cache Ø¨Ù†Ø¬Ø§Ø­",
        "min_similarity_used": min_similarity,  # âœ… Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©
        "results": {
            "similarity_cache_size": len(results.get('similarity_cache', {})),
            "word_stats_cache_size": len(results.get('word_stats_cache', {}))
        }
    }

@app.get("/performance/stats")
def get_performance_statistics():
    """
    ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    """
    return {
        "database_size": "6,236 verses",
        "faiss_ready": FAISS_INDEX is not None,
        "faiss_index_size": os.path.getsize("quran_faiss_index.bin") / 1024 / 1024 if os.path.exists("quran_faiss_index.bin") else 0,
        "fts_available": FTS_AVAILABLE,
        "similarity_cache_size": len(SIMILARITY_CACHE) if SIMILARITY_CACHE else 0,
        "word_stats_cache_size": len(WORD_STATS_CACHE) if WORD_STATS_CACHE else 0,
        "expert_mode_questions": MUTASHABIHAT_BANK['total_questions'] if MUTASHABIHAT_BANK else 0,
        "optimizations_enabled": [
            "FTS5 Live Search (5-20ms)",
            "Similarity Cache (10-50ms)", 
            "Word Statistics Cache (1-5ms)",
            "AutoComplete Suggestions",
            "LRU Cache (1000 entries)",
            "ğŸš€ Fast All Similarities (1-10s)",
            "ğŸ” Optimized Text Search (5-50ms)"
        ]
    }

# ============================================
# ğŸ“Š endpoints Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
# ============================================

@app.get("/stats/word")
def get_word_statistics(
    word: str = Query(..., min_length=1, description="Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ùˆ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ù„Ù„Ø¨Ø­Ø«"),
    limit: int = Query(100, gt=0, le=1000, description="Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    db: Session = Depends(get_db)
):
    """
    ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙƒÙ„Ù…Ø© Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø© ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù†
    
    ÙŠØ¹Ø±Ø¶:
    - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    - Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙˆØ±Ø¯Øª ÙÙŠÙ‡Ø§
    - Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙˆØ±
    - Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
    - Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª
    """
    start_time = time.time()
    
    try:
        word_clean = clean_text(word)
        
        if len(word_clean) < 2:
            raise HTTPException(status_code=400, detail="Ø§Ù„ÙƒÙ„Ù…Ø© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹")
        
        all_verses = db.query(Verse).all()
        
        matches = []
        surah_counts = {}
        juz_counts = {}
        total_count = 0
        
        for verse in all_verses:
            verse_clean = clean_text(verse.text)
            count = verse_clean.count(word_clean)
            
            if count > 0:
                matches.append({
                    'verse': verse.to_dict(),
                    'count': count
                })
                
                total_count += count
                
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø³ÙˆØ±Ø©
                surah_key = f"{verse.surah_name} ({verse.surah})"
                surah_counts[surah_key] = surah_counts.get(surah_key, 0) + count
                
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø²Ø¡
                if verse.juz:
                    juz_key = f"Ø§Ù„Ø¬Ø²Ø¡ {verse.juz}"
                    juz_counts[juz_key] = juz_counts.get(juz_key, 0) + count
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        matches.sort(key=lambda x: x['count'], reverse=True)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        surah_counts_sorted = dict(sorted(surah_counts.items(), key=lambda x: x[1], reverse=True))
        juz_counts_sorted = dict(sorted(juz_counts.items(), key=lambda x: x[1], reverse=True))
        
        elapsed = time.time() - start_time
        
        return {
            'word': word,
            'word_normalized': word_clean,
            'total_count': total_count,
            'verses_count': len(matches),
            'by_surah': surah_counts_sorted,
            'by_juz': juz_counts_sorted,
            'matches': matches[:limit],
            'search_time': f"{elapsed:.3f}s"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø©: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
# ============================================
# Ù…Ø³Ø§Ø±Ø§Øª API Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© - ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ø­Ø«
# ============================================

@app.get("/search")
def search_verses(
    q: str = Query(..., min_length=1),
    limit: int = Query(20, gt=0, le=100),
    threshold: float = Query(0.7, ge=0.05, le=1.0),  # âœ… Ø±ÙØ¹ Ù…Ù† 0.1 Ø¥Ù„Ù‰ 0.7
    highlight: bool = Query(True, description="ØªØ¸Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
    db: Session = Depends(get_db)
):
    """
    ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Ù…Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚
    ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¢Ù† Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ÙˆØ§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© + ØªØ¸Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: 10-100ms (Ù…Ø­Ø³Ù‘Ù† 10x)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø«: '{q}'")
    print(f"   Ø§Ù„Ø­Ø¯: {limit}ØŒ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {threshold}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        # ğŸŒŸ Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ÙˆØ§Ù„Ø³Ø±ÙŠØ¹
        exact_results = optimized_text_search(db, q, limit)

        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙƒØ§ÙÙŠØ©ØŒ Ù†ÙƒØªÙÙŠ Ø¨Ù‡Ø§
        if exact_results:
            # ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¸Ù„ÙŠÙ„ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
            if highlight:
                for result in exact_results:
                    result['highlighted_text'] = highlight_words_in_text(result['text'], q)
    
            elapsed = time.time() - start_time
            print(f"âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚: {len(exact_results)} Ù†ØªÙŠØ¬Ø© ÙÙŠ {elapsed:.3f}Ø«")
    
            return {
                "query": q,
                "search_time": f"{elapsed:.3f}s",
                "total_found": len(exact_results),
                "match_type": "exact",
                "results": exact_results
            }
        
        # ğŸŒŸ Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
        print("âš ï¸ Ù„Ù… ØªÙƒÙÙ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚ â€” Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†")
        fallback_results = fallback_search(db, q, limit, threshold)
        
        # ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¸Ù„ÙŠÙ„ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if highlight and 'results' in fallback_results:
            for result in fallback_results['results']:
                result['highlighted_text'] = highlight_words_in_text(result['text'], q)
        
        return fallback_results

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        fallback_results = fallback_search(db, q, limit, threshold, error=str(e))
        
        # ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¸Ù„ÙŠÙ„ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if highlight and 'results' in fallback_results:
            for result in fallback_results['results']:
                result['highlighted_text'] = highlight_words_in_text(result['text'], q)
        
        return fallback_results

# [Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¯ÙˆØ§Ù„ ÙˆØ§Ù„Ø£Ù†Ø¯Ø¨ÙˆÙŠÙ†ØªØ³ ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±]
@app.get("/similar/{verse_id}")
def get_similar_verses(
    verse_id: int = Path(...),
    limit: int = Query(10, gt=0, le=50),
    threshold: float = Query(0.4, ge=0.3, le=1.0),
    exclude_basmala: bool = Query(True),
    method: str = Query("smart", pattern="^(smart|semantic|lexical)$"),
    db: Session = Depends(get_db)
):
    """
    ğŸŒŸ Ø¬Ù„Ø¨ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ø¢ÙŠØ© Ù…Ø¹ÙŠÙ†Ø©
    
    Methods:
    - smart: Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ† Ø°ÙƒÙŠ (FAISS Ù„Ù„Ù…Ø±Ø´Ø­ÙŠÙ† + Ù„ÙØ¸ÙŠ Ù„Ù„Ø¯Ù‚Ø©) â­
    - semantic: Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS ÙÙ‚Ø·
    - lexical: Ø¨Ø­Ø« Ù„ÙØ¸ÙŠ ÙÙ‚Ø·
    """
    global FAISS_INDEX, EMBEDDING_MODEL, QURAN_IDS, QURAN_EMBEDDINGS
    
    verse = db.query(Verse).filter(Verse.id == verse_id).first()
    if not verse:
        raise HTTPException(status_code=404, detail="Ø§Ù„Ø¢ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")

    start_time = time.time()
    similarities = []

    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
    if method == "smart" and FAISS_INDEX is not None and QURAN_EMBEDDINGS is not None:
        try:
            if verse_id in QURAN_IDS:
                target_index = np.where(QURAN_IDS == verse_id)[0][0]
                target_embedding = QURAN_EMBEDDINGS[target_index:target_index+1].astype('float32')
                
                k = min(limit * 3, FAISS_INDEX.ntotal)
                distances, indices = FAISS_INDEX.search(target_embedding, k)
                
                for i, idx in enumerate(indices[0]):
                    compare_id = int(QURAN_IDS[idx])
                    
                    if compare_id == verse_id:
                        continue
                    
                    compare_verse = db.query(Verse).filter(Verse.id == compare_id).first()
                    if not compare_verse or (exclude_basmala and is_basmala_verse(compare_verse)):
                        continue
                    
                    lexical_sim = calculate_word_similarity(verse.text, compare_verse.text)
                    
                    # âœ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ 100% ØªØ´Ø§Ø¨Ù‡
                    if lexical_sim >= threshold and lexical_sim < 0.99:
                        similarities.append({
                            'verse_id': compare_id,
                            'surah': compare_verse.surah,
                            'surah_name': compare_verse.surah_name,
                            'ayah': compare_verse.ayah,
                            'text': compare_verse.text,
                            'distance': 1.0 - lexical_sim,
                            'similarity': lexical_sim,
                            'method': 'smart_hybrid'
                        })
                
                similarities.sort(key=lambda x: x['similarity'], reverse=True)
                similarities = similarities[:limit]
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†: {e}")
            method = "lexical"

    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ (fallback)
    if method == "lexical" or (method == "smart" and not similarities):
        all_verses = db.query(Verse).all()
        if exclude_basmala:
            all_verses = [v for v in all_verses if not is_basmala_verse(v)]
        
        for other_verse in all_verses:
            if other_verse.id == verse_id:
                continue
            
            similarity = calculate_word_similarity(verse.text, other_verse.text)
            
            # âœ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ 100% ØªØ´Ø§Ø¨Ù‡
            if similarity >= threshold and similarity < 0.99:
                similarities.append({
                    'verse_id': other_verse.id,
                    'surah': other_verse.surah,
                    'surah_name': other_verse.surah_name,
                    'ayah': other_verse.ayah,
                    'text': other_verse.text,
                    'distance': 1.0 - similarity,
                    'similarity': similarity,
                    'method': 'lexical'
                })
        
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        similarities = similarities[:limit]

    elapsed = time.time() - start_time
    
    return {
        'verse': verse.to_dict(),
        'similar_verses': similarities[:limit],
        'search_time': f"{elapsed:.2f}s",
        'method_used': method,
        'total_found': len(similarities)
    }

@app.get("/verse/{surah}/{ayah}")
def get_specific_verse(
    surah: int = Path(..., gt=0, le=114),
    ayah: int = Path(..., gt=0),
    db: Session = Depends(get_db)
):
    """Ø¬Ù„Ø¨ Ø¢ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©"""
    verse = db.query(Verse).filter(Verse.surah == surah, Verse.ayah == ayah).first()
    if not verse:
        raise HTTPException(status_code=404, detail=f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¢ÙŠØ© {surah}:{ayah}")
    return verse.to_dict()

@app.get("/verses", response_model=List[dict])
def get_verses(
    skip: int = Query(0, ge=0),
    limit: int = Query(10, gt=0, le=100),
    db: Session = Depends(get_db)
):
    """Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª"""
    verses = db.query(Verse).offset(skip).limit(limit).all()
    return [v.to_dict() for v in verses]

@app.get("/compare/{id1}/{id2}")
def compare_verses(
    id1: int = Path(...),
    id2: int = Path(...),
    db: Session = Depends(get_db)
):
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø¢ÙŠØªÙŠÙ†"""
    verse1 = db.query(Verse).filter(Verse.id == id1).first()
    verse2 = db.query(Verse).filter(Verse.id == id2).first()
    
    if not verse1 or not verse2:
        raise HTTPException(status_code=404, detail="Ø¢ÙŠØ© ÙˆØ§Ø­Ø¯Ø© Ø£Ùˆ ÙƒÙ„Ø§Ù‡Ù…Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")

    highlighted1, highlighted2 = highlight_differences(verse1.text, verse2.text)
    
    return {
        "verse1": verse1.to_dict(),
        "verse2": verse2.to_dict(),
        "highlighted1": highlighted1,
        "highlighted2": highlighted2,
    }

@app.get("/stats")
def get_statistics(db: Session = Depends(get_db)):
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    total_verses = db.query(Verse).count()
    surahs = db.query(Verse.surah).distinct().count()
    
    return {
        "total_verses": total_verses,
        "total_surahs": surahs,
        "faiss_ready": FAISS_INDEX is not None,
        "smart_hybrid_available": FAISS_INDEX is not None,
        "fts_available": FTS_AVAILABLE,
        "similarity_cache_size": len(SIMILARITY_CACHE) if SIMILARITY_CACHE else 0,
        "word_stats_cache_size": len(WORD_STATS_CACHE) if WORD_STATS_CACHE else 0
    }

@app.get("/all-similarities")
def get_all_similarities(
    min_similarity: float = Query(0.70, ge=0.1, le=1.0),  # âœ… ØºÙŠØ± Ù…Ù† 0.3 Ø¥Ù„Ù‰ 0.1
    limit: int = Query(100, gt=0, le=10000),
    exclude_basmala: bool = Query(True),
    surah: Optional[int] = Query(None, ge=1, le=114),
    juz: Optional[int] = Query(None, ge=1, le=30),
    third: Optional[int] = Query(None, ge=1, le=3),
    full_quran: Optional[bool] = Query(False),
    compare_surah: Optional[int] = Query(None, ge=1, le=114),
    compare_juz: Optional[int] = Query(None, ge=1, le=30),
    use_faiss: bool = Query(True),
    use_cache: bool = Query(True),
    db: Session = Depends(get_db)
):
    """
    ğŸŒŸ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ - Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    """
    
    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    if compare_surah is not None and surah is None:
        raise HTTPException(
            status_code=422, 
            detail="compare_surah requires surah to be specified"
        )
    
    if compare_juz is not None and juz is None:
        raise HTTPException(
            status_code=422, 
            detail="compare_juz requires juz to be specified"
        )
    
    print(f"\n{'='*60}")
    print(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„")
    print(f"   Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©: {'FAISS Ù…Ø³Ø±Ù‘Ø¹' if use_faiss else 'Ù„ÙØ¸ÙŠ Ø¨Ø·ÙŠØ¡'}")
    print(f"   ğŸš€ Cache Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹: {'Ù…ÙØ¹Ù„' if use_cache else 'Ù…Ø¹Ø·Ù„'}")
    print(f"   Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {min_similarity*100}%")
    print(f"   Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰: {limit} Ù†ØªÙŠØ¬Ø©")
    print(f"{'='*60}\n")

    start_time = time.time()
    
    # ============================================
    # âœ… 1. ØªØ­Ø¯ÙŠØ¯ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø« (Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©)
    # ============================================
    target_query = db.query(Verse)
    
    if full_quran:
        target_verses = target_query.order_by(Verse.id).all()
        search_scope = "Ø§Ù„Ù‚Ø±Ø¢Ù† ÙƒØ§Ù…Ù„Ø§Ù‹"
    elif third:
        if third == 1:
            juz_range = (1, 10)
            third_name = "Ø§Ù„Ø«Ù„Ø« Ø§Ù„Ø£ÙˆÙ„"
        elif third == 2:
            juz_range = (11, 20)
            third_name = "Ø§Ù„Ø«Ù„Ø« Ø§Ù„Ø«Ø§Ù†ÙŠ"
        else:
            juz_range = (21, 30)
            third_name = "Ø§Ù„Ø«Ù„Ø« Ø§Ù„Ø«Ø§Ù„Ø«"
        
        target_verses = target_query.filter(Verse.juz.between(*juz_range)).order_by(Verse.id).all()
        search_scope = f"{third_name} (Ø£Ø¬Ø²Ø§Ø¡ {juz_range[0]}-{juz_range[1]})"
    elif surah:
        target_verses = target_query.filter(Verse.surah == surah).order_by(Verse.id).all()
        search_scope = f"Ø³ÙˆØ±Ø© {surah}"
    elif juz:
        target_verses = target_query.filter(Verse.juz == juz).order_by(Verse.id).all()
        search_scope = f"Ø§Ù„Ø¬Ø²Ø¡ {juz}"
    else:
        target_verses = target_query.order_by(Verse.id).all()
        search_scope = "Ø§Ù„Ù‚Ø±Ø¢Ù† ÙƒØ§Ù…Ù„Ø§Ù‹"

    if exclude_basmala:
        target_verses = [v for v in target_verses if not is_basmala_verse(v)]

    print(f"ğŸ“Š Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø«: {search_scope} ({len(target_verses)} Ø¢ÙŠØ©)")

    # ============================================
    # âœ… 2. ØªØ­Ø¯ÙŠØ¯ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© (Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©)
    # ============================================
    compare_query = db.query(Verse)
    
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯ Ù†Ø·Ø§Ù‚ Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù…   Ø§Ù„Ù‚Ø±Ø¢Ù† ÙƒØ§Ù…Ù„Ø§Ù‹ 
    if compare_surah:
        compare_verses = compare_query.filter(Verse.surah == compare_surah).order_by(Verse.id).all()
        compare_scope = f"Ø³ÙˆØ±Ø© {compare_surah}"
    elif compare_juz:
        compare_verses = compare_query.filter(Verse.juz == compare_juz).order_by(Verse.id).all()
        compare_scope = f"Ø§Ù„Ø¬Ø²Ø¡ {compare_juz}"
    else:
        # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø±Ø¢Ù† ÙƒØ§Ù…Ù„Ø§Ù‹ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        compare_verses = compare_query.order_by(Verse.id).all()
        compare_scope = "Ø§Ù„Ù‚Ø±Ø¢Ù† ÙƒØ§Ù…Ù„Ø§Ù‹"
    
    if exclude_basmala:
        compare_verses = [v for v in compare_verses if not is_basmala_verse(v)]
    
    print(f"ğŸ“Š Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: {compare_scope} ({len(compare_verses)} Ø¢ÙŠØ©)")

    if len(target_verses) < 1:
        return {
            "total_found": 0,
            "similarities": [],
            "search_time": "0.00s",
            "min_similarity": min_similarity,
            "search_scope": search_scope,
            "compare_scope": compare_scope,
            "method": "faiss" if use_faiss else "lexical"
        }

    # ============================================
    # ğŸš€ 3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
    # ============================================
    if use_cache and SIMILARITY_CACHE and len(SIMILARITY_CACHE) > 0:
        print("ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… Similarity Cache Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹...")
        similarities = fast_all_similarities_from_cache(
            db, target_verses, compare_verses, min_similarity, limit, exclude_basmala
        )
        method_used = "cache_accelerated"
    
    # ============================================
    # âš¡ 4. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø³Ø±Ù‘Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† cache Ù…ØªØ§Ø­Ø§Ù‹)
    # ============================================
    elif use_faiss and FAISS_INDEX is not None:
        print("âš¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ù„Ù„ØªØ³Ø±ÙŠØ¹...")
        
        similarities = []
        seen_pairs = set()
        
        # âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© IDs Ù„Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© (Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹)
        compare_verse_ids = set(v.id for v in compare_verses)
        
        total_verses = len(target_verses)
        
        for idx, target_verse in enumerate(target_verses):
            if (idx + 1) % 50 == 0:
                elapsed_so_far = time.time() - start_time
                print(f"   Ø§Ù„ØªÙ‚Ø¯Ù…: {idx + 1}/{total_verses} Ø¢ÙŠØ© ({elapsed_so_far:.1f}Ø«ØŒ {len(similarities)} Ù…ØªØ´Ø§Ø¨Ù‡)")
            
            if target_verse.id not in QURAN_IDS:
                continue
            
            try:
                target_index = np.where(QURAN_IDS == target_verse.id)[0][0]
                target_embedding = QURAN_EMBEDDINGS[target_index:target_index+1].astype('float32')
                
                k = min(50, FAISS_INDEX.ntotal)
                distances, indices = FAISS_INDEX.search(target_embedding, k)
                
                for i, idx_faiss in enumerate(indices[0]):
                    compare_id = int(QURAN_IDS[idx_faiss])
                    
                    # âœ… ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ÙØ³ Ø§Ù„Ø¢ÙŠØ©
                    if compare_id == target_verse.id:
                        continue
                    
                    # âœ… ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¢ÙŠØ© Ø®Ø§Ø±Ø¬ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
                    if compare_id not in compare_verse_ids:
                        continue
                    
                    pair = tuple(sorted([target_verse.id, compare_id]))
                    if pair in seen_pairs:
                        continue
                    
                    compare_verse = db.query(Verse).filter(Verse.id == compare_id).first()
                    if not compare_verse or (exclude_basmala and is_basmala_verse(compare_verse)):
                        continue
                    
                    lexical_sim = calculate_word_similarity(target_verse.text, compare_verse.text)
                    
                    # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ù€ 100% Ù…Ø¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø°ÙƒÙŠØ© 
                    if lexical_sim >= min_similarity and not is_excluded_100_percent_match(target_verse.text, compare_verse.text):
                        seen_pairs.add(pair)
                        similarities.append({
                            'verse1': target_verse.to_dict(),
                            'verse2': compare_verse.to_dict(),
                            'similarity': lexical_sim,
                            'score_percent': int(lexical_sim * 100)
                        })
                        
                        if len(similarities) >= limit:
                            break
                
                if len(similarities) >= limit:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¢ÙŠØ© {target_verse.id}: {e}")
                continue
        
        method_used = "faiss_accelerated"
    
    # ============================================
    # ğŸ¢ 5. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ Ø§Ù„Ø¨Ø·ÙŠØ¡ (fallback)
    # ============================================
    else:
        print("ğŸ¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ...")
        
        # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: ØªØ¹Ø±ÙŠÙ compare_verse_ids Ø§Ù„Ù…ÙÙ‚ÙˆØ¯
        compare_verse_ids = set(v.id for v in compare_verses)

        similarities = []
        seen_pairs = set()
        
        # âœ… Ø¥Ø¶Ø§ÙØ© Ø¯ÙŠØ¨Ø§Ø¬ Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø§ ÙŠØ­Ø¯Ø«
        total_comparisons = 0
        passed_threshold = 0

        for target_verse in target_verses:
            for compare_verse in compare_verses:
                total_comparisons += 1
                
                # ØªØ®Ø·ÙŠ Ù†ÙØ³ Ø§Ù„Ø¢ÙŠØ©
                if target_verse.id >= compare_verse.id:
                    continue
                
                # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¢ÙŠØ© ÙÙŠ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
                if compare_verse.id not in compare_verse_ids:
                    continue
                
                similarity = calculate_word_similarity(target_verse.text, compare_verse.text)

                # âœ… Ø¯ÙŠØ¨Ø§Ø¬: Ø·Ø¨Ø§Ø¹Ø© Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª
                if total_comparisons <= 10:  # Ø£ÙˆÙ„ 10 Ù…Ù‚Ø§Ø±Ù†Ø§Øª ÙÙ‚Ø·
                    print(f"   ğŸ” Ù…Ù‚Ø§Ø±Ù†Ø© {total_comparisons}: {similarity:.3f}")

                
                # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ù€ 100% Ù…Ø¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø°ÙƒÙŠØ©  
                if similarity >= min_similarity and not is_excluded_100_percent_match(target_verse.text, compare_verse.text):
                    passed_threshold += 1
                    pair = tuple(sorted([target_verse.id, compare_verse.id]))
                    if pair not in seen_pairs:
                        seen_pairs.add(pair)
                        similarities.append({
                            'verse1': target_verse.to_dict(),
                            'verse2': compare_verse.to_dict(),
                            'similarity': similarity,
                            'score_percent': int(similarity * 100)
                        })
                    
                    if len(similarities) >= limit:
                        break
            
            if len(similarities) >= limit:
                break
        # âœ… Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙØ¸ÙŠ:")
        print(f"   ğŸ” Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª: {total_comparisons}")
        print(f"   âœ… ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø­Ø¯ ({min_similarity}): {passed_threshold}")
        print(f"   ğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {len(similarities)}")


        method_used = "lexical"

    similarities.sort(key=lambda x: x['similarity'], reverse=True)
    elapsed = time.time() - start_time

    print(f"\n{'='*60}")
    print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ ÙÙŠ {elapsed:.2f}Ø«. {len(similarities)} Ø²ÙˆØ¬.")
    print(f"{'='*60}\n")

    return {
        "total_found": len(similarities),
        "similarities": similarities,
        "search_time": f"{elapsed:.2f}s",
        "min_similarity": min_similarity,
        "search_scope": search_scope,
        "compare_scope": compare_scope,
        "method": method_used,
        "cache_used": use_cache and SIMILARITY_CACHE and len(SIMILARITY_CACHE) > 0  # âœ… Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Cache
    }

# ============================================
# ğŸ® Quiz Endpoints - FIXED COMPLETELY + ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ†
# ============================================

def get_quiz_scope_filter(scope_type: str, scope_value: str):
    """ØªÙ†Ø´Ø¦ Ø´Ø±Ø· Ø§Ù„ÙÙ„ØªØ±Ø© Ù„Ø¬Ù„Ø¨ Ø¢ÙŠØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    if scope_type == 'all':
        return True
    elif scope_type == 'juz' and scope_value.isdigit():
        return Verse.juz == int(scope_value)
    elif scope_type == 'surah' and scope_value.isdigit():
        return Verse.surah == int(scope_value)
    elif scope_type == 'thulth' and scope_value.isdigit():
        thulth = int(scope_value)
        if thulth == 1:
            return Verse.juz.between(1, 10)
        elif thulth == 2:
            return Verse.juz.between(11, 20)
        elif thulth == 3:
            return Verse.juz.between(21, 30)
    return True

# ============================================
# ğŸ† Ø¯ÙˆØ§Ù„ Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Quiz
# ============================================

def get_expert_surah_name_question(db: Session, scope_filter):
    """ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±: Ø³Ø¤Ø§Ù„ Ù…Ø§ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© Ù…Ø¹ Ø¢ÙŠØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ù†Ùƒ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢ÙŠØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©
    if not MUTASHABIHAT_BANK:
        return get_quiz_question({"question_type": "surah_name", "expert_mode": False}, db)
    
    questions = MUTASHABIHAT_BANK['questions']
    
    for attempt in range(10):
        question_data = random.choice(questions)
        
        if len(question_data['verses']) < 2:
            continue
        
        valid_verses = []
        for verse_info in question_data['verses']:
            verse = db.query(Verse).filter(
                Verse.surah == verse_info['surah'],
                Verse.ayah == verse_info['ayah']
            ).first()
            
            if verse and (scope_filter == True or db.query(Verse).filter(Verse.id == verse.id, scope_filter).first()):
                valid_verses.append(verse)
        
        if len(valid_verses) >= 2:
            correct_verse = random.choice(valid_verses)
            
            # Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ©
            other_surahs = list(set([v.surah for v in valid_verses if v.surah != correct_verse.surah]))
            if len(other_surahs) < 3:
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø³ÙˆØ± ÙƒØ§ÙÙŠØ©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø³ÙˆØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
                other_surahs = db.query(Verse.surah).filter(Verse.surah != correct_verse.surah).distinct().limit(3).all()
                other_surahs = [s[0] for s in other_surahs]
            
            wrong_choices = []
            for surah_id in other_surahs[:3]:
                surah_name = db.query(Verse.surah_name).filter(Verse.surah == surah_id).first()[0]
                wrong_choices.append(surah_name)
            
            choices = [correct_verse.surah_name] + wrong_choices
            random.shuffle(choices)
            
            print(f"ğŸ† expert_surah_name: {correct_verse.surah_name}")
            print(f"   Category: {question_data['category']}")
            print(f"   Choices: {choices}")
            
            return {
                "question_type": "surah_name",
                "question_text": f"{correct_verse.text}\n\n(ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ† - {question_data['category']})",
                "correct_answer": correct_verse.surah_name,
                "choices": choices,
                "verse_info": {
                    "surah_name": correct_verse.surah_name,
                    "surah": correct_verse.surah,
                    "ayah": correct_verse.ayah
                },
                "expert_mode": True,
                "category": question_data['category']
            }
    
    return get_quiz_question({"question_type": "surah_name", "expert_mode": False}, db)

def get_expert_word_choice_question(db: Session, scope_filter, threshold: float):
    """ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
    if not MUTASHABIHAT_BANK:
        return get_word_choice_question(db, scope_filter, threshold)
    
    questions = MUTASHABIHAT_BANK['questions']
    
    for attempt in range(10):
        question_data = random.choice(questions)
        
        if len(question_data['verses']) < 2:
            continue
        
        valid_verses = []
        for verse_info in question_data['verses']:
            verse = db.query(Verse).filter(
                Verse.surah == verse_info['surah'],
                Verse.ayah == verse_info['ayah']
            ).first()
            
            if verse and (scope_filter == True or db.query(Verse).filter(Verse.id == verse.id, scope_filter).first()):
                valid_verses.append(verse)
        
        if len(valid_verses) >= 1:
            correct_verse = random.choice(valid_verses)
            words = correct_verse.text.split()
            
            if len(words) < 5:
                continue
            
            # Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡
            word_index = random.randint(1, len(words) - 2)
            correct_word = words[word_index]
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„
            question_text = " ".join(words[:word_index]) + " (___) " + " ".join(words[word_index+1:])
            
            # Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©
            similar_words = []
            for v in valid_verses:
                if v.id != correct_verse.id:
                    v_words = v.text.split()
                    if len(v_words) > word_index:
                        similar_words.append(v_words[word_index])
            
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ù†Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
            if len(similar_words) < 3:
                distractors = get_word_distractors(db, correct_word, correct_verse.surah, 3)
                options = [correct_word] + distractors
            else:
                options = [correct_word] + similar_words[:3]
            
            options = list(set(options))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
            random.shuffle(options)
            
            print(f"ğŸ† expert_word_choice: {correct_word}")
            print(f"   Category: {question_data['category']}")
            print(f"   Options: {options}")
            
            return {
                "question_type": "word_choice",
                "question_text": f"{question_text}\n\n(ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ† - {question_data['category']})",
                "correct_answer": correct_word,
                "verse_info": {
                    "surah_name": correct_verse.surah_name,
                    "surah": correct_verse.surah,
                    "ayah": correct_verse.ayah
                },
                "options": options,
                "expert_mode": True,
                "category": question_data['category']
            }
    
    return get_word_choice_question(db, scope_filter, threshold)

def get_expert_continue_question(db: Session, scope_filter):
    """ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±: Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¢ÙŠØ© Ù…Ø¹ Ø¢ÙŠØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
    if not MUTASHABIHAT_BANK:
        return get_quiz_question({"question_type": "continue", "expert_mode": False}, db)
    
    questions = MUTASHABIHAT_BANK['questions']
    
    for attempt in range(10):
        question_data = random.choice(questions)
        
        if len(question_data['verses']) < 1:
            continue
        
        valid_verses = []
        for verse_info in question_data['verses']:
            verse = db.query(Verse).filter(
                Verse.surah == verse_info['surah'],
                Verse.ayah == verse_info['ayah']
            ).first()
            
            if verse and (scope_filter == True or db.query(Verse).filter(Verse.id == verse.id, scope_filter).first()):
                valid_verses.append(verse)
        
        if len(valid_verses) >= 1:
            correct_verse = random.choice(valid_verses)
            words = correct_verse.text.split()
            
            if len(words) < 6:
                continue
            
            # Ø­Ø°Ù 1-3 ÙƒÙ„Ù…Ø§Øª
            num_words_to_hide = min(3, max(1, len(words) // 5))
            max_start = len(words) - num_words_to_hide - 1
            start_index = random.randint(1, max(1, max_start))
            
            hidden_words = words[start_index:start_index + num_words_to_hide]
            correct_answer = ' '.join(hidden_words)
            
            question_words = (
                words[:start_index] + 
                ['____'] + 
                words[start_index + num_words_to_hide:]
            )
            question_text = ' '.join(question_words)
            
            print(f"ğŸ† expert_continue: {correct_answer}")
            print(f"   Category: {question_data['category']}")
            print(f"   Hidden: {num_words_to_hide} ÙƒÙ„Ù…Ø§Øª")
            
            return {
                "question_type": "continue",
                "question_text": f"{question_text}\n\n(ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± ğŸ† - {question_data['category']})",
                "correct_answer": correct_answer,
                "choices": [],
                "verse_info": {
                    "surah_name": correct_verse.surah_name,
                    "surah": correct_verse.surah,
                    "ayah": correct_verse.ayah
                },
                "expert_mode": True,
                "category": question_data['category']
            }
    
    return get_quiz_question({"question_type": "continue", "expert_mode": False}, db)

@app.post("/quiz/get_question")
def get_quiz_question(data: dict, db: Session = Depends(get_db)):
    """
    âœ… Quiz Ù…Ø­Ø³Ù‘Ù† - Ù…Ø¹ Ø¯Ø¹Ù… ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„
    
    Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:
    - continue: Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¢ÙŠØ© (Ø­Ø°Ù 1-3 ÙƒÙ„Ù…Ø§Øª)
    - word_choice: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© (Ø­Ø°Ù 1-3 ÙƒÙ„Ù…Ø§Øª + Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©)
    - distinguish: ØªÙ…ÙŠÙŠØ² Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª (Ø¢ÙŠØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© 85-95%)
    - surah_name: Ù…Ø§ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø©ØŸ
    
    ğŸ† ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±:
    - expert_mode: true â†’ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ù†Ùƒ "Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙƒÙ„Ù…Ø©" Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
    """
    scope_type = data.get('scope_type', 'all')
    scope_value = data.get('scope_value', '1')
    question_type = data.get('question_type', 'continue')
    threshold = data.get('threshold', 0.75)
    expert_mode = data.get('expert_mode', False)  # ğŸ† Ø¬Ø¯ÙŠØ¯

    print(f"\n{'='*60}")
    print(f"ğŸ® Quiz Request:")
    print(f"   Type: {question_type}")
    print(f"   Scope: {scope_type} = {scope_value}")
    print(f"   Threshold: {threshold}")
    print(f"   ğŸ† Expert Mode: {expert_mode}")  # ğŸ† Ø¬Ø¯ÙŠØ¯
    print(f"{'='*60}\n")

    try:
        scope_filter = get_quiz_scope_filter(scope_type, scope_value)
        
        # ============================================
        # ğŸ† Ø¥Ø°Ø§ ÙƒØ§Ù† ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± Ù…ÙØ¹Ù„Ø§Ù‹
        # ============================================
        if expert_mode:
            print("ğŸ† ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ± Ù…ÙØ¹Ù‘Ù„ - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ‚Ø¯Ù…Ø©")
            
            # 1. Ø§Ø®ØªØ¨Ø§Ø±: Ù…Ø§ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø©ØŸ (ÙˆØ¶Ø¹ Ø®Ø¨ÙŠØ±)
            if question_type == 'surah_name':
                return get_expert_surah_name_question(db, scope_filter)
            
            # 2. Ø§Ø®ØªØ¨Ø§Ø±: Ù…ÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª (ÙˆØ¶Ø¹ Ø®Ø¨ÙŠØ±)
            elif question_type == 'distinguish':
                return get_expert_distinguish_question(db, scope_filter)
            
            # 3. Ø§Ø®ØªØ¨Ø§Ø±: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø© (ÙˆØ¶Ø¹ Ø®Ø¨ÙŠØ±)
            elif question_type == 'word_choice':
                return get_expert_word_choice_question(db, scope_filter, threshold)
            
            # 4. Ø§Ø®ØªØ¨Ø§Ø±: Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¢ÙŠØ© (ÙˆØ¶Ø¹ Ø®Ø¨ÙŠØ±)
            elif question_type == 'continue':
                return get_expert_continue_question(db, scope_filter)
        
        # ============================================
        # âœ… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠ (Ø¨Ø¯ÙˆÙ† Ø®Ø¨ÙŠØ±)
        # ============================================
        
        # 1. Ø§Ø®ØªØ¨Ø§Ø±: Ù…Ø§ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø©ØŸ
        if question_type == 'surah_name':
            random_verse = db.query(Verse).filter(scope_filter).order_by(func.random()).first()
    
            if not random_verse:
                raise HTTPException(status_code=404, detail="No verses found")
    
            wrong_surahs = db.query(Verse.surah_name).\
                filter(Verse.surah != random_verse.surah).\
                distinct().\
                order_by(func.random()).\
                limit(3).all()
    
            wrong_choices = [s[0] for s in wrong_surahs if s[0]][:3]
            choices = [random_verse.surah_name] + wrong_choices
            random.shuffle(choices)
    
            print(f"âœ… surah_name: {random_verse.surah_name}")
            print(f"   Choices: {choices}\n")
    
            return {
                "question_type": "surah_name",
                "question_text": random_verse.text,
                "correct_answer": random_verse.surah_name,
                "choices": choices,
                "verse_info": {
                    "surah_name": random_verse.surah_name,
                    "surah": random_verse.surah,
                    "ayah": random_verse.ayah
                }
            }
        
        # 2. Ø§Ø®ØªØ¨Ø§Ø±: Ù…ÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª (Ø¹Ø§Ø¯ÙŠ)
        elif question_type == 'distinguish':
            return get_distinguish_question(db, scope_filter, threshold)
        
        # 3. Ø§Ø®ØªØ¨Ø§Ø±: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø©
        elif question_type == 'word_choice':
            return get_word_choice_question(db, scope_filter, threshold)
        
        # 4. Ø§Ø®ØªØ¨Ø§Ø±: Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¢ÙŠØ©
        random_verse = db.query(Verse).filter(scope_filter).order_by(func.random()).first()
        
        if not random_verse:
            raise HTTPException(status_code=404, detail="No verses found")
        
        words = random_verse.text.split()
        
        if len(words) < 6:
            return get_quiz_question(data, db)
        
        # Ø­Ø°Ù 1-3 ÙƒÙ„Ù…Ø§Øª
        num_words_to_hide = min(3, max(1, len(words) // 5))
        
        max_start = len(words) - num_words_to_hide - 1
        start_index = random.randint(1, max(1, max_start))
        
        hidden_words = words[start_index:start_index + num_words_to_hide]
        correct_answer = ' '.join(hidden_words)
        
        question_words = (
            words[:start_index] + 
            ['____'] + 
            words[start_index + num_words_to_hide:]
        )
        question_text = ' '.join(question_words)
        
        print(f"âœ… continue:")
        print(f"   Hidden: {num_words_to_hide} ÙƒÙ„Ù…Ø§Øª")
        print(f"   Question: {question_text}")
        print(f"   Correct: {correct_answer}\n")
        
        return {
            "question_type": "continue",
            "question_text": question_text,
            "correct_answer": correct_answer,
            "choices": [],
            "verse_info": {
                "surah_name": random_verse.surah_name,
                "surah": random_verse.surah,
                "ayah": random_verse.ayah
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø­Ø±Ø¬ ÙÙŠ Quiz: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")
    
# ============================================
# ğŸ†• endpoint Ø¬Ø¯ÙŠØ¯: Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ø¹ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª
# ============================================

@app.get("/verses/random-with-similarities")
def get_random_verses_with_similarities(
    limit: int = Query(10, gt=0, le=20),
    min_similarity: float = Query(0.85, ge=0.6, le=0.99),
    db: Session = Depends(get_db)
):
    """
    Ø¬Ù„Ø¨ Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ© Ù…Ø¹ Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù‡Ø§
    
    Parameters:
    - limit: Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ 10)
    - min_similarity: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡ (Ø§ÙØªØ±Ø§Ø¶ÙŠ 85%)
    
    Returns:
    - Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ù† Ø³ÙˆØ± Ù…Ø®ØªÙ„ÙØ©ØŒ ÙƒÙ„ Ø¢ÙŠØ© Ù„Ù‡Ø§ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª
    """
    global FAISS_INDEX, QURAN_EMBEDDINGS, QURAN_IDS
    
    start_time = time.time()
    print(f"\n{'='*60}")
    print(f"ğŸ² Ø¬Ù„Ø¨ {limit} Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ø¹ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª {min_similarity*100}%+")
    print(f"{'='*60}\n")
    
    try:
        # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¢ÙŠØ§Øª
        all_verses = db.query(Verse).all()
        
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨Ø³Ù…Ù„Ø§Øª
        all_verses = [v for v in all_verses if not is_basmala_verse(v)]
        
        if len(all_verses) < limit:
            return {
                "verses": [v.to_dict() for v in all_verses],
                "search_time": "0.00s",
                "total_found": len(all_verses)
            }
        
        # Ø®Ù„Ø· Ø§Ù„Ø¢ÙŠØ§Øª
        random.shuffle(all_verses)
        
        selected_verses = []
        used_surahs = set()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢ÙŠØ§Øª Ù…Ù†Ø§Ø³Ø¨Ø©
        for verse in all_verses:
            # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø³ÙˆØ±Ø© Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨Ø§Ù„ÙØ¹Ù„
            if verse.surah in used_surahs:
                continue
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©
            has_similarities = False
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ø¥Ø°Ø§ Ù…ØªØ§Ø­
            if FAISS_INDEX is not None and verse.id in QURAN_IDS:
                try:
                    target_index = np.where(QURAN_IDS == verse.id)[0][0]
                    target_embedding = QURAN_EMBEDDINGS[target_index:target_index+1].astype('float32')
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† 10 Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù…Ø­ØªÙ…Ù„Ø©
                    k = min(10, FAISS_INDEX.ntotal)
                    distances, indices = FAISS_INDEX.search(target_embedding, k)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„ÙØ¸ÙŠØ©
                    for idx in indices[0]:
                        compare_id = int(QURAN_IDS[idx])
                        if compare_id == verse.id:
                            continue
                        
                        compare_verse = db.query(Verse).filter(Verse.id == compare_id).first()
                        if not compare_verse:
                            continue
                        
                        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ
                        similarity = calculate_word_similarity(verse.text, compare_verse.text)
                        
                        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù…Ù†Ø§Ø³Ø¨Ø©
                        if min_similarity <= similarity < 0.99:
                            has_similarities = True
                            break
                    
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ FAISS Ù„Ù„Ø¢ÙŠØ© {verse.id}: {e}")
                    continue
            
            # Fallback: Ø¨Ø­Ø« Ù„ÙØ¸ÙŠ Ø³Ø±ÙŠØ¹
            else:
                # Ù†Ø¨Ø­Ø« ÙÙŠ Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (100 Ø¢ÙŠØ©) Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
                sample_verses = random.sample(all_verses, min(100, len(all_verses)))
                
                for other_verse in sample_verses:
                    if other_verse.id == verse.id:
                        continue
                    
                    similarity = calculate_word_similarity(verse.text, other_verse.text)
                    
                    if min_similarity <= similarity < 0.99:
                        has_similarities = True
                        break
            
            # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…ØªØ´Ø§Ø¨Ù‡Ø§ØªØŒ Ø£Ø¶Ù Ø§Ù„Ø¢ÙŠØ©
            if has_similarities:
                selected_verses.append(verse)
                used_surahs.add(verse.surah)
                
                print(f"âœ“ ÙˆØ¬Ø¯Øª Ø§Ù„Ø¢ÙŠØ© {len(selected_verses)}: {verse.surah_name} ({verse.surah}:{verse.ayah})")
                
                # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                if len(selected_verses) >= limit:
                    break
        
        elapsed = time.time() - start_time
        
        print(f"\n{'='*60}")
        print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(selected_verses)} Ø¢ÙŠØ§Øª ÙÙŠ {elapsed:.2f}Ø«")
        print(f"{'='*60}\n")
        
        return {
            "verses": [v.to_dict() for v in selected_verses],
            "search_time": f"{elapsed:.2f}s",
            "total_found": len(selected_verses),
            "min_similarity": min_similarity
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback: Ø¬Ù„Ø¨ Ø¢ÙŠØ§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¹Ø§Ø¯ÙŠØ©
        random_verses = db.query(Verse).order_by(func.random()).limit(limit).all()
        return {
            "verses": [v.to_dict() for v in random_verses],
            "search_time": "0.00s",
            "total_found": len(random_verses),
            "error": "ØªÙ… Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©"
        }


# Ø£Ø¶Ù Ù‡Ø°Ø§ endpoint Ù…Ø¤Ù‚Øª Ù„Ù„ØªØ­Ù‚Ù‚
@app.get("/debug/check-cache")
def debug_check_cache(
    surah: int = Query(..., ge=1, le=114),
    ayah: int = Query(..., ge=1),
    db: Session = Depends(get_db)
):
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¢ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´"""
    verse = db.query(Verse).filter(Verse.surah == surah, Verse.ayah == ayah).first()
    
    if not verse:
        raise HTTPException(status_code=404, detail="Ø§Ù„Ø¢ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
    
    cache_entries = []
    if verse.id in SIMILARITY_CACHE:
        for sim in SIMILARITY_CACHE[verse.id]:
            cache_entries.append({
                "compare_verse": f"{sim['surah']}:{sim['ayah']}",
                "similarity": sim['similarity'],
                "text_preview": sim['text'][:50] + "..."
            })
    
    return {
        "verse": f"{verse.surah}:{verse.ayah}",
        "in_cache": verse.id in SIMILARITY_CACHE,
        "cache_entries_count": len(cache_entries),
        "cache_entries": cache_entries[:10]  # Ø£ÙˆÙ„ 10 Ø¥Ø¯Ø®Ø§Ù„Ø§Øª ÙÙ‚Ø·
    }

# ============================================
# âš™ï¸ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
# ============================================
if __name__ == "__main__":
    import uvicorn
    try:
        db_session = next(get_db())
        init_db(db_session)
        if EMBEDDING_AVAILABLE:
            initialize_search_engine(db_session)
        db_session.close()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}")

    print(f"\n--- ğŸ’» Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… Ø¹Ù„Ù‰ http://{HOST}:{PORT} ---")
    
    # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    uvicorn.run(
        "main:app", 
        host=HOST,           # âœ… Ù…Ù† Environment
        port=PORT,           # âœ… Ù…Ù† Environment
        reload=not PRODUCTION  # âœ… ØªØ¹Ø·ÙŠÙ„ reload ÙÙŠ Production
    )